[
  {
    "type": "code",
    "source": "!pip install -q transformers datasets accelerate bitsandbytes pysam pandas pyarrow fastparquet\n!apt-get install -q samtools\n!pip install tqdm\n",
    "outputs": [
      {
        "type": "stream",
        "text": "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\nRequirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.67.1)\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "print(\"Hello\")",
    "outputs": [
      {
        "type": "stream",
        "text": "Hello\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "!pip install evaluate\n!pip install scikit-learn\n!pip install matplotlib\n",
    "outputs": [
      {
        "type": "stream",
        "text": "Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (4.4.2)\nRequirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (2.2.6)\nRequirement already satisfied: dill in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (2.3.3)\nRequirement already satisfied: requests>=2.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.20.3)\nRequirement already satisfied: pyarrow>=21.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\nRequirement already satisfied: anyio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.1)\nRequirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: idna in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\nRequirement already satisfied: h11>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.6.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2025.3)\nRequirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.6\n"
      },
      {
        "type": "stream",
        "text": "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy>=1.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\nCollecting scipy>=1.8.0 (from scikit-learn)\n  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting joblib>=1.2.0 (from scikit-learn)\n  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\nCollecting threadpoolctl>=3.1.0 (from scikit-learn)\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nDownloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m186.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\nDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m207.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0\nCollecting matplotlib\n  Downloading matplotlib-3.10.8-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\nCollecting contourpy>=1.0.1 (from matplotlib)\n  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\nCollecting cycler>=0.10 (from matplotlib)\n  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting fonttools>=4.22.0 (from matplotlib)\n  Downloading fonttools-4.61.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (114 kB)\nCollecting kiwisolver>=1.3.1 (from matplotlib)\n  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\nRequirement already satisfied: numpy>=1.23 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (2.2.6)\nRequirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (25.0)\nCollecting pillow>=8 (from matplotlib)\n  Downloading pillow-12.1.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\nCollecting pyparsing>=3 (from matplotlib)\n  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nDownloading matplotlib-3.10.8-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m151.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\nDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\nDownloading fonttools-4.61.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.9 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m169.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading pillow-12.1.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m153.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\nInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]7\u001b[0m [matplotlib]\n\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.1.0 pyparsing-3.3.1\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ==== CELL 3: Re-tokenize + normalize extra features + build torch-format HF Datasets ====\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nNUM_PROCS = min(8, max(1, (os.cpu_count() or 8) - 2))\nMAX_LEN = 1000\nMODEL_NAME = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n\n# --------------- feature list (same order everywhere) ---------------\nfeature_cols = [\n    \"gnomad_af\",\n    \"GERP++_RS_rankscore\",\n    \"GERP_91_mammals_rankscore\",\n    \"phyloP100way_vertebrate_rankscore\",\n    \"phyloP470way_mammalian_rankscore\",\n    \"phyloP17way_primate_rankscore\",\n    \"phastCons100way_vertebrate_rankscore\",\n    \"phastCons470way_mammalian_rankscore\",\n    \"phastCons17way_primate_rankscore\",\n]\n\nprint(\"NUM_PROCS:\", NUM_PROCS, \"MAX_LEN:\", MAX_LEN)\n\n# 1) load\ntrain_df = pd.read_parquet(\"Balanced_300k_SEQUENCES.parquet\")\ntest_df  = pd.read_parquet(\"test_enriched_SEQUENCES.parquet\")\nprint(\"Loaded:\", len(train_df), \"train |\", len(test_df), \"test\")\n\n# basic check\nfor c in feature_cols:\n    if c not in train_df.columns:\n        raise RuntimeError(f\"Missing column in train dataframe: {c}\")\n\n# 2) compute train mean/std for normalization (stable)\ntrain_feat = train_df[feature_cols].astype(float).fillna(0.0)\nfeat_means = train_feat.mean(axis=0).astype(np.float32).values\nfeat_stds  = train_feat.std(axis=0).replace(0, 1).astype(np.float32).values\n\nprint(\"Feature means/std computed.\")\n\n# 3) tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\n# 4) helper to build dataset\nfrom datasets import Dataset\n\ndef build_and_tokenize(df, split_name=\"train\"):\n    # keep only required columns\n    sub = df[[\"raw_sequence\", \"clean_label\"] + feature_cols].copy()\n    sub[\"labels\"] = (sub[\"clean_label\"] == \"Pathogenic\").astype(np.int64)\n    sub = sub.drop(columns=[\"clean_label\"])\n    ds = Dataset.from_pandas(sub, preserve_index=False)\n\n    # tokenization\n    def tok_fn(batch):\n        enc = tokenizer(\n            batch[\"raw_sequence\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN\n        )\n        return enc\n\n    ds = ds.map(\n        tok_fn,\n        batched=True,\n        batch_size=128,\n        num_proc=NUM_PROCS,\n        remove_columns=[\"raw_sequence\"],\n        desc=f\"Tokenizing {split_name}\"\n    )\n\n    # pack & normalize features\n    def pack_norm_features(batch):\n        # stack columns in correct order and normalize by train stats\n        arrs = [np.array(batch[c], dtype=np.float32) for c in feature_cols]\n        stacked = np.stack(arrs, axis=1)  # (batch, n_features)\n        # normalize\n        stacked = (stacked - feat_means) / feat_stds\n        return {\"features\": stacked.tolist()}\n\n    ds = ds.map(\n        pack_norm_features,\n        batched=True,\n        batch_size=512,\n        num_proc=NUM_PROCS,\n        remove_columns=feature_cols,\n        desc=f\"Packing/normalizing features {split_name}\"\n    )\n\n    # set torch format (ensures Trainer receives torch tensors)\n    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"features\", \"labels\"])\n\n    # sample sanity\n    s = ds[0]\n    print(f\"[{split_name}] sample keys: {list(s.keys())}\")\n    print(f\"[{split_name}] shapes/dtypes: input_ids={s['input_ids'].shape}/{s['input_ids'].dtype}, features={len(s['features'])}/{s['features'].dtype}, labels={s['labels'].dtype}\")\n    return ds\n\ntrain_fast = build_and_tokenize(train_df, \"train\")\ntest_fast  = build_and_tokenize(test_df, \"test\")\n\nprint(\"\u2705 Cell 3 complete. Train size:\", len(train_fast), \"Test size:\", len(test_fast))\n",
    "outputs": [
      {
        "type": "stream",
        "text": "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
      },
      {
        "type": "stream",
        "text": "NUM_PROCS: 8 MAX_LEN: 1000\nLoaded: 299999 train | 14073 test\nFeature means/std computed.\n"
      },
      {
        "type": "stream",
        "text": "Setting TOKENIZERS_PARALLELISM=false for forked processes.\nTokenizing train (num_proc=8): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 299999/299999 [01:30<00:00, 3321.33 examples/s]\nSetting TOKENIZERS_PARALLELISM=false for forked processes.\nPacking/normalizing features train (num_proc=8): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 299999/299999 [00:30<00:00, 9687.11 examples/s] \n"
      },
      {
        "type": "stream",
        "text": "[train] sample keys: ['labels', 'input_ids', 'attention_mask', 'features']\n[train] shapes/dtypes: input_ids=torch.Size([1000])/torch.int64, features=9/torch.float32, labels=torch.int64\n"
      },
      {
        "type": "stream",
        "text": "Setting TOKENIZERS_PARALLELISM=false for forked processes.\nTokenizing test (num_proc=8): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14073/14073 [00:05<00:00, 2406.31 examples/s]\nSetting TOKENIZERS_PARALLELISM=false for forked processes.\nPacking/normalizing features test (num_proc=8): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14073/14073 [00:02<00:00, 4779.46 examples/s]"
      },
      {
        "type": "stream",
        "text": "[test] sample keys: ['labels', 'input_ids', 'attention_mask', 'features']\n[test] shapes/dtypes: input_ids=torch.Size([1000])/torch.int64, features=9/torch.float32, labels=torch.int64\n\u2705 Cell 3 complete. Train size: 299999 Test size: 14073\n"
      },
      {
        "type": "stream",
        "text": "\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ===== CELL 3.5 \u2014 SYSTEM & VARIABLE VERIFICATION =====\nimport torch\nimport transformers\nfrom transformers import TrainingArguments\nimport psutil\n\nprint(\"\ud83d\udd0d RUNNING PRE-FLIGHT CHECKS...\")\n\n# 1. HARDWARE VERIFICATION (Confirm H200)\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"\u2705 GPU DETECTED: {gpu_name}\")\n    print(f\"   VRAM: {vram_gb:.2f} GB\")\n    \n    if \"H200\" in gpu_name or \"H100\" in gpu_name:\n        print(\"   \ud83d\ude80 GOD MODE HARDWARE CONFIRMED.\")\n    else:\n        print(\"   \u26a0\ufe0f WARNING: You are not on H100/H200. Adjust batch sizes!\")\nelse:\n    raise RuntimeError(\"\u274c NO GPU DETECTED! Training will fail.\")\n\n# 2. DATASET VERIFICATION\ntry:\n    print(f\"\u2705 Train Set: {len(train_fast)} samples\")\n    print(f\"\u2705 Test Set:  {len(test_fast)} samples\")\n    \n    # Check Column Names (Must match model expectations)\n    sample = train_fast[0]\n    required_keys = [\"input_ids\", \"attention_mask\", \"features\", \"labels\"]\n    missing = [k for k in required_keys if k not in sample.keys()]\n    \n    if missing:\n        raise ValueError(f\"\u274c DATASET MISSING KEYS: {missing}\")\n    print(\"   Structure: OK (Torch tensors present)\")\n    \nexcept NameError:\n    raise RuntimeError(\"\u274c Datasets 'train_fast' or 'test_fast' not found. Did Cell 3 run?\")\n\n# 3. HUGGING FACE ARGUMENT CHECK (The \"eval_strategy\" Bug Fix)\n# Newer transformers use 'eval_strategy', older use 'evaluation_strategy'\nimport inspect\nargs_sig = inspect.signature(TrainingArguments.__init__)\nvalid_params = args_sig.parameters.keys()\n\nif \"eval_strategy\" in valid_params:\n    EVAL_STRATEGY_KEY = \"eval_strategy\"\n    print(\"\u2705 Detected modern Transformers: Using 'eval_strategy'\")\nelse:\n    EVAL_STRATEGY_KEY = \"evaluation_strategy\"\n    print(\"\u26a0\ufe0f Detected older Transformers: Using 'evaluation_strategy'\")\n\nprint(f\"\\nSystem Ready. Using strategy key: '{EVAL_STRATEGY_KEY}'\")",
    "outputs": [
      {
        "type": "stream",
        "text": "\ud83d\udd0d RUNNING PRE-FLIGHT CHECKS...\n\u2705 GPU DETECTED: NVIDIA H200\n   VRAM: 150.12 GB\n   \ud83d\ude80 GOD MODE HARDWARE CONFIRMED.\n\u2705 Train Set: 299999 samples\n\u2705 Test Set:  14073 samples\n   Structure: OK (Torch tensors present)\n\u2705 Detected modern Transformers: Using 'eval_strategy'\n\nSystem Ready. Using strategy key: 'eval_strategy'\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ===== CELL 4 \u2014 SAFE BIOLOGICAL FINE-TUNE (NT-500M, A100) \u2014 CORRECTED =====\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom transformers import AutoModel, TrainingArguments, Trainer, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport evaluate\nfrom safetensors.torch import save_file\n\n# 1. Optimizations for A100 (Ampere)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n# -----------------------------\n# Metrics\n# -----------------------------\nauc_metric = evaluate.load(\"roc_auc\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Softmax implementation for stability\n    exp = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n    probs = exp[:, 1] / exp.sum(axis=1)\n    return auc_metric.compute(prediction_scores=probs, references=labels)\n\n# -----------------------------\n# Load NT-500M Encoder\n# -----------------------------\nMODEL_NAME = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\nFEATURE_DIM = len(feature_cols) # Ensure feature_cols is defined in previous cells\nNUM_LABELS = 2\n\nencoder = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\nencoder.gradient_checkpointing_enable()\nprint(\"\ud83d\udd25 Encoder gradient checkpointing enabled\")\n\nhidden_size = encoder.config.hidden_size\n\n# -----------------------------\n# Hybrid Classifier (Corrected Pooling)\n# -----------------------------\nclass SequenceClassificationWithFeatures(nn.Module):\n    def __init__(self, encoder, hidden_size, feature_dim, num_labels):\n        super().__init__()\n        self.encoder = encoder\n        self.feature_dim = feature_dim\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size + feature_dim, hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, num_labels)\n        )\n\n    def forward(self, input_ids=None, attention_mask=None, features=None, labels=None):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n\n        # Robust pooling strategy\n        if hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n            pooled = out.pooler_output\n        else:\n            # Mean pooling if pooler_output is missing\n            last = out.last_hidden_state\n            mask = attention_mask.unsqueeze(-1)\n            pooled = (last * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n\n        # Ensure features match device and dtype\n        features = features.to(pooled.device).to(pooled.dtype)\n        \n        # Concatenate and Classify\n        x = torch.cat([pooled, features], dim=1)\n        logits = self.classifier(x)\n\n        loss = None\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n\n        return SequenceClassifierOutput(loss=loss, logits=logits)\n\nmodel = SequenceClassificationWithFeatures(\n    encoder=encoder,\n    hidden_size=hidden_size,\n    feature_dim=FEATURE_DIM,\n    num_labels=NUM_LABELS\n)\n\n# -----------------------------\n# Data collator\n# -----------------------------\ndef data_collator(batch):\n    return {\n        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n        # Improved tensor creation to handle varying input types safely\n        \"features\": torch.stack([torch.as_tensor(b[\"features\"], dtype=torch.float32) for b in batch]),\n        \"labels\": torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long),\n    }\n\n# -----------------------------\n# Training config\n# -----------------------------\n# Effective Batch Size = 64 * 2 = 128 (Ideal for NT-500M stability)\ntraining_args = TrainingArguments(\n    output_dir=\"./PathoPreter_v1_A100\",\n\n    per_device_train_batch_size=64,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=32,\n\n    learning_rate=2e-5,\n    num_train_epochs=2,\n    weight_decay=0.02,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.05,\n\n    bf16=True,   # Essential for A100\n    fp16=False,\n\n    optim=\"adamw_torch_fused\",\n    max_grad_norm=0.5,\n\n    dataloader_num_workers=8,\n    dataloader_pin_memory=True,\n\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_steps=200,\n\n    load_best_model_at_end=True,\n    metric_for_best_model=\"roc_auc\",\n    greater_is_better=True,\n    save_total_limit=2,\n\n    report_to=\"none\"\n)\n\ntorch.cuda.empty_cache()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_fast,\n    eval_dataset=test_fast,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\nprint(\"\ud83d\ude80 NT-500M biological fine-tuning started (A100, fresh run)\")\ntrainer.train()\n\n# ========================================================\n# \ud83d\udcbe TITANIUM SAVE \u2014 A100 MODEL\n# ========================================================\n\nprint(\"\\n\ud83d\udcbe SAVING FINAL A100 MODEL...\")\n\nsave_path = \"./PathoPreter_Final_A100\"\nos.makedirs(save_path, exist_ok=True)\n\n# 1. Get the base encoder config\nraw_model = model.encoder\nif hasattr(raw_model, \"_orig_mod\"):\n    raw_model = raw_model._orig_mod\n\n# 2. INJECT CUSTOM DIMENSIONS INTO CONFIG\n# This is crucial for easy inference loading later!\nraw_model.config.custom_feature_dim = FEATURE_DIM\nraw_model.config.custom_num_labels = NUM_LABELS\nraw_model.config.architectures = [\"SequenceClassificationWithFeatures\"]\n\n# 3. Save Config and Tokenizer\nraw_model.config.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\n# 4. Save Weights (Cleaning prefixes)\nraw_state = model.state_dict()\nclean_state = {}\n\nfor k, v in raw_state.items():\n    k = k.replace(\"_orig_mod.\", \"\")\n    # Remove 'encoder.' prefix to match HF format, but KEEP 'classifier' keys\n    if k.startswith(\"encoder.\"):\n        clean_state[k.replace(\"encoder.\", \"\", 1)] = v\n    else:\n        clean_state[k] = v\n\nsave_file(clean_state, os.path.join(save_path, \"model.safetensors\"))\n\n# 5. Save Scaling Factors\nnp.save(os.path.join(save_path, \"feat_means.npy\"), feat_means)\nnp.save(os.path.join(save_path, \"feat_stds.npy\"), feat_stds)\n\nprint(f\"\u2705 A100 MODEL SAVED \u2192 {save_path}\")\nprint(\"   (Config updated with 'custom_feature_dim' for easier inference)\")\nprint(\"\ud83d\ude80 A100 RUN COMPLETE.\")",
    "outputs": [
      {
        "type": "stream",
        "text": "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
      },
      {
        "type": "stream",
        "text": "\ud83d\udd25 Encoder gradient checkpointing enabled\n\ud83d\ude80 NT-500M biological fine-tuning started (A100, fresh run)\n"
      },
      {
        "type": "stream",
        "text": "\n\ud83d\udcbe SAVING FINAL A100 MODEL...\n\u2705 A100 MODEL SAVED \u2192 ./PathoPreter_Final_A100\n   (Config updated with 'custom_feature_dim' for easier inference)\n\ud83d\ude80 A100 RUN COMPLETE.\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# # ==========================================\n# # \ud83d\uded1 EMERGENCY SAVE: RECOVER BEST CHECKPOINT\n# # ==========================================\n# # Run this ONLY if you interrupted training manually!\n\n# import os\n# import shutil\n# import torch\n# import numpy as np\n# from safetensors.torch import load_file, save_file\n\n# print(\"\ud83d\udea8 INTERRUPT DETECTED. INITIATING RECOVERY...\")\n\n# # 1. FIND THE BEST CHECKPOINT\n# # The trainer object tracks this automatically\n# if hasattr(trainer, \"state\") and trainer.state.best_model_checkpoint:\n#     best_ckpt_path = trainer.state.best_model_checkpoint\n#     print(f\"\ud83c\udfc6 Best Model found at: {best_ckpt_path}\")\n# else:\n#     # Fallback: If run was too short to find a 'best', use the latest output dir\n#     print(\"\u26a0\ufe0f No 'Best' checkpoint recorded (maybe too early?). Checking output dir...\")\n#     best_ckpt_path = trainer.args.output_dir\n#     # Find latest checkpoint folder manually if needed\n#     if not os.path.exists(os.path.join(best_ckpt_path, \"model.safetensors\")):\n#         # Sort folders by number\n#         subdirs = [x for x in os.listdir(best_ckpt_path) if \"checkpoint\" in x]\n#         if subdirs:\n#             subdirs.sort(key=lambda x: int(x.split(\"-\")[-1]))\n#             best_ckpt_path = os.path.join(best_ckpt_path, subdirs[-1])\n#             print(f\"\ud83d\udc49 Fallback: Using latest checkpoint {best_ckpt_path}\")\n\n# # 2. LOAD THE BEST WEIGHTS\n# print(f\"\ud83d\udce5 Loading weights from: {best_ckpt_path}\")\n# weights_file = os.path.join(best_ckpt_path, \"model.safetensors\")\n\n# if not os.path.exists(weights_file):\n#     # Try pytorch_model.bin if safetensors missing\n#     weights_file = os.path.join(best_ckpt_path, \"pytorch_model.bin\")\n#     if not os.path.exists(weights_file):\n#         raise FileNotFoundError(f\"\u274c No weights found in {best_ckpt_path}\")\n#     # Load bin\n#     state_dict = torch.load(weights_file, map_location=\"cpu\")\n# else:\n#     # Load safetensors\n#     state_dict = load_file(weights_file)\n\n# # 3. CLEAN & STRIP PREFIXES (The Titanium Fix)\n# clean_state_dict = {}\n# print(\"\u2702\ufe0f  Cleaning weights for deployment...\")\n\n# for key, value in state_dict.items():\n#     # Remove compiler prefix\n#     key = key.replace(\"_orig_mod.\", \"\")\n    \n#     if key.startswith(\"encoder.\"):\n#         # \"encoder.embeddings...\" -> \"embeddings...\" (For AutoModel)\n#         new_key = key.replace(\"encoder.\", \"\", 1)\n#         clean_state_dict[new_key] = value\n#     else:\n#         # Keep classifier keys\n#         clean_state_dict[key] = value\n\n# # 4. SAVE FINAL MODEL\n# final_path = \"./PathoPreter_Final_H200_Best\"\n# os.makedirs(final_path, exist_ok=True)\n\n# print(f\"\ud83d\udcbe Saving cleaned model to: {final_path}\")\n# save_file(clean_state_dict, os.path.join(final_path, \"model.safetensors\"))\n\n# # Save Config & Tokenizer (From memory is fine)\n# raw_model = model.encoder if hasattr(model, \"encoder\") else model\n# if hasattr(raw_model, \"_orig_mod\"): raw_model = raw_model._orig_mod\n# raw_model.config.save_pretrained(final_path)\n# tokenizer.save_pretrained(final_path)\n\n# # Save Stats (Global variables)\n# np.save(os.path.join(final_path, \"feat_means.npy\"), feat_means)\n# np.save(os.path.join(final_path, \"feat_stds.npy\"), feat_stds)\n\n# print(\"\\n\u2705 SUCCESS!\")\n# print(f\"   Saved Best AUC Checkpoint from: {best_ckpt_path}\")\n# print(f\"   Location: {final_path}\")\n# print(\"   You can now download this folder.\")",
    "outputs": []
  },
  {
    "type": "code",
    "source": "# ============================================================\n# CELL: Model Calibration (Fixed Version)\n# ============================================================\n\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import brier_score_loss\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np\n\n# 1. Generate Predictions first! (This fixes the NameError)\nprint(\"\ud83d\ude80 Running final evaluation to get probabilities...\")\npredictions_output = trainer.predict(test_fast)\n\n# Extract Labels (y_true) and Probabilities (y_probs)\ny_true = predictions_output.label_ids\nlogits = predictions_output.predictions\n\n# Convert Logits -> Probabilities (Softmax)\ny_probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n\nprint(f\"\u2705 Extracted {len(y_probs)} predictions.\")\n\n# 2. Calibration Plot Function\ndef plot_calibration_report(y_true, y_probs, model_name=\"PathoPreter\"):\n    # Calculate Brier Score (Lower is better)\n    brier = brier_score_loss(y_true, y_probs)\n    \n    # Generate Calibration Curve\n    prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n    \n    # Plotting\n    plt.figure(figsize=(10, 7))\n    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n    plt.plot(prob_pred, prob_true, marker='s', label=f'{model_name} (Brier: {brier:.4f})', color='darkorange')\n    \n    plt.xlabel('Mean Predicted Probability', fontsize=12)\n    plt.ylabel('Fraction of Positives (Actual)', fontsize=12)\n    plt.title(f'\ud83e\uddec Clinical Reliability Diagram: {model_name}', fontsize=14)\n    plt.legend(loc='lower right')\n    plt.grid(alpha=0.3)\n    \n    plt.text(0.05, 0.9, \"Above line = Under-confident\\nBelow line = Over-confident\", \n             fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n    \n    plt.show()\n    \n    print(f\"\ud83d\udcca Brier Score: {brier:.4f}\")\n    if brier < 0.1:\n        print(\"\u2705 EXCELLENT: Model is highly reliable for clinical use.\")\n    elif brier < 0.15:\n        print(\"\u2705 GOOD: Reliable enough for research.\")\n    elif brier < 0.2:\n        print(\"\u26a0\ufe0f MODERATE: Probabilities slightly skewed.\")\n    else:\n        print(\"\u274c POOR: Use rankings only.\")\n\n# 3. Run it\nplot_calibration_report(y_true, y_probs)",
    "outputs": [
      {
        "type": "stream",
        "text": "\ud83d\ude80 Running final evaluation to get probabilities...\n"
      },
      {
        "type": "stream",
        "text": "\u2705 Extracted 14073 predictions.\n"
      },
      {
        "type": "stream",
        "text": "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 129516 (\\N{DNA DOUBLE HELIX}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n"
      },
      {
        "type": "stream",
        "text": "\ud83d\udcca Brier Score: 0.1297\n\u2705 GOOD: Reliable enough for research.\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ============================================================\n# CELL FINAL \u2014 PathoPreter vs DBNSFP SOTA Leaderboard\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# ---------------------------------------------\n# 1. Run model on HF test dataset\n# ---------------------------------------------\npreds = trainer.predict(test_fast)   # use test_fast if you used cell-3 pipeline\nlogits = preds.predictions\nlabels = preds.label_ids\n\n# Convert logits -> probability of Pathogenic\nprobs = torch.softmax(torch.tensor(logits), dim=-1)[:,1].cpu().numpy()\n\n# ---------------------------------------------\n# 2. Recover original variant IDs\n# ---------------------------------------------\n# test_fast was created from test_df in cell-3, in same order\ntest_df = pd.read_parquet(\"test_enriched_SEQUENCES.parquet\").reset_index(drop=True)\n\nassert len(test_df) == len(probs), \"\u274c Row mismatch between model and dataframe\"\n\npred_df = pd.DataFrame({\n    \"variant_id\": test_df[\"variant_id\"].values,\n    \"y_true\": labels,\n    \"PathoPreter\": probs\n})\n\n# ---------------------------------------------\n# 3. Collect DBNSFP benchmark tools\n# ---------------------------------------------\nbench_cols = [\n    \"CADD_raw_rankscore\",\n    \"REVEL_rankscore\",\n    \"AlphaMissense_rankscore\",\n    \"PrimateAI_rankscore\",\n    \"MPC_rankscore\",\n    \"ClinPred_rankscore\",\n    \"BayesDel_addAF_rankscore\",\n    \"EVE_rankscore\",\n    \"ESM1b_rankscore\"\n]\n\nbench_df = test_df[[\"variant_id\"] + bench_cols].copy()\n\n# ---------------------------------------------\n# 4. Merge safely (alignment lock)\n# ---------------------------------------------\nmerged = pred_df.merge(bench_df, on=\"variant_id\", how=\"inner\")\n\nprint(\"Aligned evaluation rows:\", len(merged))\n\ny = merged[\"y_true\"].values\n\n# ---------------------------------------------\n# 5. Confusion Matrix (PathoPreter)\n# ---------------------------------------------\ny_pred = (merged[\"PathoPreter\"] > 0.5).astype(int)\n\ncm = confusion_matrix(y, y_pred)\ndisp = ConfusionMatrixDisplay(cm, display_labels=[\"Benign\", \"Pathogenic\"])\ndisp.plot(cmap=\"Blues\")\nplt.title(\"PathoPreter Confusion Matrix (Unseen Test)\")\nplt.show()\n\n# ---------------------------------------------\n# 6. Full SOTA Leaderboard\n# ---------------------------------------------\nmodels = {\n    \"PathoPreter\": merged[\"PathoPreter\"],\n    \"CADD\": merged[\"CADD_raw_rankscore\"],\n    \"REVEL\": merged[\"REVEL_rankscore\"],\n    \"AlphaMissense\": merged[\"AlphaMissense_rankscore\"],\n    \"PrimateAI\": merged[\"PrimateAI_rankscore\"],\n    \"MPC\": merged[\"MPC_rankscore\"],\n    \"ClinPred\": merged[\"ClinPred_rankscore\"],\n    \"BayesDel\": merged[\"BayesDel_addAF_rankscore\"],\n    \"EVE\": merged[\"EVE_rankscore\"],\n    \"ESM1b\": merged[\"ESM1b_rankscore\"],\n}\n\nresults = []\nfor name, scores in models.items():\n    auc = roc_auc_score(y, scores)\n    pr  = average_precision_score(y, scores)\n    results.append((name, auc, pr))\n\nresults.sort(key=lambda x: x[1], reverse=True)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83c\udfc6 PATHOGENICITY SOTA LEADERBOARD \u2014 ClinVar-Unseen Test\")\nprint(\"=\"*70)\nprint(f\"{'Model':20s} | {'ROC-AUC':8s} | {'PR-AUC':8s}\")\nprint(\"-\"*70)\n\nfor name, auc, pr in results:\n    print(f\"{name:20s} | {auc:8.4f} | {pr:8.4f}\")\n\nprint(\"=\"*70)\n\n# ---------------------------------------------\n# 7. Gains vs CADD (industry baseline)\n# ---------------------------------------------\nscores = dict((n,a) for n,a,_ in results)\n\nprint(f\"\\n\ud83d\udd25 PathoPreter gain over CADD: {scores['PathoPreter'] - scores['CADD']:+.4f} ROC-AUC\")\nprint(f\"\ud83d\udd25 PathoPreter gain over REVEL: {scores['PathoPreter'] - scores['REVEL']:+.4f} ROC-AUC\")\nprint(f\"\ud83d\udd25 PathoPreter gain over AlphaMissense: {scores['PathoPreter'] - scores['AlphaMissense']:+.4f} ROC-AUC\")\n",
    "outputs": [
      {
        "type": "stream",
        "text": "Aligned evaluation rows: 14221\n"
      },
      {
        "type": "stream",
        "text": "\n======================================================================\n\ud83c\udfc6 PATHOGENICITY SOTA LEADERBOARD \u2014 ClinVar-Unseen Test\n======================================================================\nModel                | ROC-AUC  | PR-AUC  \n----------------------------------------------------------------------\nPathoPreter          |   0.9186 |   0.9284\nBayesDel             |   0.5949 |   0.7097\nCADD                 |   0.5921 |   0.7079\nClinPred             |   0.5886 |   0.6095\nAlphaMissense        |   0.5879 |   0.6050\nREVEL                |   0.5847 |   0.6026\nESM1b                |   0.5763 |   0.5938\nPrimateAI            |   0.5661 |   0.5850\nMPC                  |   0.5596 |   0.5821\nEVE                  |   0.5223 |   0.5504\n======================================================================\n\n\ud83d\udd25 PathoPreter gain over CADD: +0.3265 ROC-AUC\n\ud83d\udd25 PathoPreter gain over REVEL: +0.3339 ROC-AUC\n\ud83d\udd25 PathoPreter gain over AlphaMissense: +0.3307 ROC-AUC\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "!pip install seaborn",
    "outputs": [
      {
        "type": "stream",
        "text": "Collecting seaborn\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (2.2.6)\nRequirement already satisfied: pandas>=1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (2.3.3)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (3.10.8)\nRequirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\nRequirement already satisfied: pillow>=8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.0)\nRequirement already satisfied: pyparsing>=3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.3)\nRequirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\nDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\nInstalling collected packages: seaborn\nSuccessfully installed seaborn-0.13.2\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# 1. Create the dataframe from the 'results' list you already have\n# (This fixes the NameError)\nleaderboard_df = pd.DataFrame(results, columns=[\"Model\", \"ROC-AUC\", \"PR-AUC\"])\n\n# 2. Define the plotting function\ndef plot_sota_gains(df):\n    # Sort for plotting\n    df_plot = df.sort_values(\"ROC-AUC\", ascending=True)\n    \n    plt.figure(figsize=(10, 6))\n    \n    # Highlight PathoPreter in Red/Orange, others in Blue\n    colors = ['#A1C9F4' if x != 'PathoPreter' else '#FF9F9B' for x in df_plot['Model']]\n    \n    bars = plt.barh(df_plot['Model'], df_plot['ROC-AUC'], color=colors, edgecolor='black')\n    \n    # Add values on the bars\n    for bar in bars:\n        width = bar.get_width()\n        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n                 f'{width:.4f}', va='center', fontweight='bold')\n\n    plt.axvline(x=0.5, color='red', linestyle='--', label='Random Guess (0.5)')\n    plt.title(\"\ud83c\udfc6 PathoPreter v1 vs. Global Benchmarks (ROC-AUC)\", fontsize=14)\n    plt.xlabel(\"ROC-AUC Score\", fontsize=12)\n    plt.xlim(0.5, 1.0) # Zoom in to show the contrast clearly\n    plt.grid(axis='x', linestyle=':', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n# 3. Plot it\nplot_sota_gains(leaderboard_df)",
    "outputs": [
      {
        "type": "stream",
        "text": "/tmp/ipykernel_10337/1626609002.py:31: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "!pip install -q transformers datasets accelerate bitsandbytes pysam pandas pyarrow fastparquet\n!apt-get install -q samtools\n!pip install tqdm\n",
    "outputs": [
      {
        "type": "stream",
        "text": "  error: subprocess-exited-with-error\n  \n  \u00d7 Getting requirements to build wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [34 lines of output]\n      # pysam: Cython 3.2.4 is available - using cythonize if necessary\n      # pysam: htslib mode is shared\n      # pysam: HTSLIB_CONFIGURE_OPTIONS=None\n      '.' is not recognized as an internal or external command,\n      operable program or batch file.\n      '.' is not recognized as an internal or external command,\n      operable program or batch file.\n      # pysam: htslib configure options: None\n      Traceback (most recent call last):\n        File \"c:\\Users\\yrohi\\Documents\\CODING\\PROJECTS\\Pathopreter_xgboost\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n          main()\n        File \"c:\\Users\\yrohi\\Documents\\CODING\\PROJECTS\\Pathopreter_xgboost\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n        File \"c:\\Users\\yrohi\\Documents\\CODING\\PROJECTS\\Pathopreter_xgboost\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 143, in get_requires_for_build_wheel\n          return hook(config_settings)\n        File \"C:\\Users\\yrohi\\AppData\\Local\\Temp\\pip-build-env-9qb6yr4x\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n          return self._get_build_requires(config_settings, requirements=[])\n        File \"C:\\Users\\yrohi\\AppData\\Local\\Temp\\pip-build-env-9qb6yr4x\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n          self.run_setup()\n        File \"C:\\Users\\yrohi\\AppData\\Local\\Temp\\pip-build-env-9qb6yr4x\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n          super().run_setup(setup_script=setup_script)\n        File \"C:\\Users\\yrohi\\AppData\\Local\\Temp\\pip-build-env-9qb6yr4x\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n          exec(code, locals())\n        File \"<string>\", line 468, in <module>\n        File \"<string>\", line 81, in run_make_print_config\n        File \"C:\\Users\\yrohi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 421, in check_output\n          return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n        File \"C:\\Users\\yrohi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 503, in run\n          with Popen(*popenargs, **kwargs) as process:\n        File \"C:\\Users\\yrohi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 971, in __init__\n          self._execute_child(args, executable, preexec_fn, close_fds,\n        File \"C:\\Users\\yrohi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 1456, in _execute_child\n          hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n      FileNotFoundError: [WinError 2] The system cannot find the file specified\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nERROR: Failed to build 'pysam' when getting requirements to build wheel\n'apt-get' is not recognized as an internal or external command,\noperable program or batch file.\n"
      },
      {
        "type": "stream",
        "text": "Requirement already satisfied: tqdm in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (4.67.1)\nRequirement already satisfied: colorama in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "!pip install seaborn\n",
    "outputs": [
      {
        "type": "stream",
        "text": "Collecting seaborn\n  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from seaborn) (1.26.4)\nRequirement already satisfied: pandas>=1.2 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from seaborn) (2.1.4)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from seaborn) (3.10.8)\nRequirement already satisfied: contourpy>=1.0.1 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\nRequirement already satisfied: pillow>=8 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\nRequirement already satisfied: pyparsing>=3 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: tzdata>=2022.1 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.3)\nRequirement already satisfied: six>=1.5 in c:\\users\\yrohi\\documents\\coding\\projects\\pathopreter_xgboost\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\nUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\nInstalling collected packages: seaborn\nSuccessfully installed seaborn-0.13.2\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ============================================================\n# CELL: BENCHMARK ACCURACY & CONFUSION MATRIX (robust)\n# ============================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_recall_curve,\n    f1_score,\n    average_precision_score,\n)\n\n# -------------- config --------------\nTEST_FILE = \"test_enriched_SEQUENCES.parquet\"\n\nbenchmarks = [\n    \"CADD_raw_rankscore\", \"REVEL_rankscore\", \"AlphaMissense_rankscore\",\n    \"PrimateAI_rankscore\", \"MPC_rankscore\", \"ClinPred_rankscore\",\n    \"BayesDel_addAF_rankscore\", \"EVE_rankscore\", \"ESM1b_rankscore\"\n]\n\n# -------------- load --------------\ndf = pd.read_parquet(TEST_FILE)\nprint(\"Loaded test rows:\", len(df))\n\n# -------------- clean & map labels --------------\n# find label column\nlabel_col = None\nfor c in (\"clean_label\", \"label\", \"labels\"):\n    if c in df.columns:\n        label_col = c\n        break\nif label_col is None:\n    raise KeyError(\"Could not find 'label', 'clean_label', or 'labels' column in test file.\")\n\ndef map_label_val(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower()\n    if s in (\"pathogenic\", \"path\", \"p\", \"1\", \"true\", \"t\", \"yes\", \"y\"):\n        return 1\n    if s in (\"benign\", \"benign \", \"b\", \"0\", \"false\", \"f\", \"no\", \"n\"):\n        return 0\n    # try numeric\n    try:\n        nv = float(s)\n        if nv == 1.0: return 1\n        if nv == 0.0: return 0\n    except:\n        pass\n    return np.nan\n\nraw_labels = df[label_col]\ny_true = raw_labels.map(map_label_val)\n\n# drop unknowns\nif y_true.isnull().any():\n    drop_count = int(y_true.isnull().sum())\n    print(f\"\u26a0\ufe0f Dropping {drop_count} rows with unmappable labels.\")\n    valid_mask = ~y_true.isnull()\n    df = df.loc[valid_mask].reset_index(drop=True)\n    y_true = y_true.loc[valid_mask].reset_index(drop=True)\nelse:\n    y_true = y_true.reset_index(drop=True)\n\ny_true = y_true.astype(int)\nprint(\"Final eval rows:\", len(y_true))\nif len(np.unique(y_true)) < 2:\n    raise RuntimeError(\"Need at least two label classes in test set for evaluation.\")\n\n# -------------- evaluate each benchmark --------------\nresults = []\nconfusion_matrices = {}\n\nfor tool in benchmarks:\n    if tool not in df.columns:\n        print(f\"\u26a0\ufe0f Skipping {tool} (not present in parquet).\")\n        continue\n\n    scores = df[tool].astype(float).copy()\n    # fill NaN with median (stable)\n    if scores.isna().any():\n        median_val = float(scores.median())\n        scores = scores.fillna(median_val)\n\n    # handle constant scores (no thresholds)\n    unique_vals = np.unique(scores)\n    if len(unique_vals) == 1:\n        # fallback threshold = median (or 0.5) and compute metrics\n        best_thresh = unique_vals[0]\n        y_pred = (scores >= best_thresh).astype(int)\n        acc = accuracy_score(y_true, y_pred)\n        f1_best = f1_score(y_true, y_pred, zero_division=0)\n        results.append({\n            \"Model\": tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\"),\n            \"Accuracy\": acc,\n            \"Best_Threshold\": float(best_thresh),\n            \"F1_Score\": float(f1_best),\n            \"AP\": float(average_precision_score(y_true, scores))\n        })\n        confusion_matrices[tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\")] = confusion_matrix(y_true, y_pred)\n        print(f\"   {tool}: constant scores \u2014 using threshold {best_thresh:.4f}\")\n        continue\n\n    # compute PR curve then choose threshold maximizing F1 (properly index thresholds)\n    precision, recall, thresholds = precision_recall_curve(y_true, scores)\n    # precision,recall length = len(thresholds)+1\n    f1_all = (2 * precision * recall) / (precision + recall + 1e-12)\n    # only consider indices that have associated thresholds => 1..end\n    if len(thresholds) > 0:\n        idx_rel = np.argmax(f1_all[1:])  # best among entries that map to thresholds\n        best_idx = idx_rel + 1           # absolute index into precision/recall arrays\n        best_thresh = float(thresholds[best_idx - 1])\n    else:\n        # very rare: fallback\n        best_thresh = float(np.median(scores))\n\n    y_pred = (scores >= best_thresh).astype(int)\n    acc = accuracy_score(y_true, y_pred)\n    f1_best = f1_score(y_true, y_pred, zero_division=0)\n    ap = average_precision_score(y_true, scores)\n\n    results.append({\n        \"Model\": tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\"),\n        \"Accuracy\": acc,\n        \"Best_Threshold\": best_thresh,\n        \"F1_Score\": float(f1_best),\n        \"AP\": float(ap)\n    })\n    confusion_matrices[tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\")] = confusion_matrix(y_true, y_pred)\n\n    print(f\"   {tool}: Acc={acc:.4f} F1={f1_best:.4f} AP={ap:.4f} thresh={best_thresh:.4f}\")\n\n# -------------- report --------------\nif not results:\n    print(\"\u274c No benchmarks found.\")\nelse:\n    res_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83c\udfc6 BENCHMARK LEADERBOARD (thresholds optimized for F1)\")\n    print(\"=\" * 60)\n    display_cols = [\"Model\", \"Accuracy\", \"Best_Threshold\", \"F1_Score\", \"AP\"]\n    print(res_df[display_cols].to_string(index=False, formatters={\n        'Accuracy': '{:.2%}'.format,\n        'Best_Threshold': '{:.4f}'.format,\n        'F1_Score': '{:.4f}'.format,\n        'AP': '{:.4f}'.format\n    }))\n\n    # plot confusion matrices in grid\n    n_tools = len(res_df)\n    cols = 3\n    rows = (n_tools + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))\n    axes = axes.flatten()\n\n    for i, row in res_df.iterrows():\n        name = row[\"Model\"]\n        cm = confusion_matrices.get(name)\n        ax = axes[i]\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax,\n                    xticklabels=[\"Pred Benign\", \"Pred Patho\"], yticklabels=[\"Act Benign\", \"Act Patho\"])\n        ax.set_title(f\"{name}\\nAcc: {row['Accuracy']:.1%}  F1: {row['F1_Score']:.3f}\")\n\n    # hide unused axes\n    for j in range(i + 1, len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n",
    "outputs": [
      {
        "type": "stream",
        "text": "Loaded test rows: 14073\nFinal eval rows: 14073\n   CADD_raw_rankscore: Acc=0.5280 F1=0.6790 AP=0.7048 thresh=0.2114\n   REVEL_rankscore: Acc=0.5326 F1=0.6799 AP=0.6004 thresh=0.5393\n   AlphaMissense_rankscore: Acc=0.5302 F1=0.6792 AP=0.6035 thresh=0.2655\n   PrimateAI_rankscore: Acc=0.5205 F1=0.6749 AP=0.5813 thresh=0.2336\n   MPC_rankscore: Acc=0.5136 F1=0.6707 AP=0.5786 thresh=0.2331\n   ClinPred_rankscore: Acc=0.5393 F1=0.6837 AP=0.6066 thresh=0.2707\n   BayesDel_addAF_rankscore: Acc=0.5383 F1=0.6838 AP=0.7063 thresh=0.2284\n   EVE_rankscore: Acc=0.5039 F1=0.6689 AP=0.5466 thresh=0.0639\n   ESM1b_rankscore: Acc=0.5241 F1=0.6742 AP=0.5911 thresh=0.4397\n\n============================================================\n\ud83c\udfc6 BENCHMARK LEADERBOARD (thresholds optimized for F1)\n============================================================\n         Model Accuracy Best_Threshold F1_Score     AP\n      ClinPred   53.93%         0.2707   0.6837 0.6066\nBayesDel_addAF   53.83%         0.2284   0.6838 0.7063\n         REVEL   53.26%         0.5393   0.6799 0.6004\n AlphaMissense   53.02%         0.2655   0.6792 0.6035\n          CADD   52.80%         0.2114   0.6790 0.7048\n         ESM1b   52.41%         0.4397   0.6742 0.5911\n     PrimateAI   52.05%         0.2336   0.6749 0.5813\n           MPC   51.36%         0.2331   0.6707 0.5786\n           EVE   50.39%         0.0639   0.6689 0.5466\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "#same test on train dataset\n# ============================================================\n# CELL: BENCHMARK ACCURACY & CONFUSION MATRIX (robust)\n# ============================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_recall_curve,\n    f1_score,\n    average_precision_score,\n)\n\n# -------------- config --------------\nTEST_FILE = \"train_modified.parquet\"\n\nbenchmarks = [\n    \"CADD_raw_rankscore\", \"REVEL_rankscore\", \"AlphaMissense_rankscore\",\n    \"PrimateAI_rankscore\", \"MPC_rankscore\", \"ClinPred_rankscore\",\n    \"BayesDel_addAF_rankscore\", \"EVE_rankscore\", \"ESM1b_rankscore\"\n]\n\n# -------------- load --------------\ndf = pd.read_parquet(TEST_FILE)\nprint(\"Loaded test rows:\", len(df))\n\n# -------------- clean & map labels --------------\n# find label column\nlabel_col = None\nfor c in (\"clean_label\", \"label\", \"labels\"):\n    if c in df.columns:\n        label_col = c\n        break\nif label_col is None:\n    raise KeyError(\"Could not find 'label', 'clean_label', or 'labels' column in test file.\")\n\ndef map_label_val(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower()\n    if s in (\"pathogenic\", \"path\", \"p\", \"1\", \"true\", \"t\", \"yes\", \"y\"):\n        return 1\n    if s in (\"benign\", \"benign \", \"b\", \"0\", \"false\", \"f\", \"no\", \"n\"):\n        return 0\n    # try numeric\n    try:\n        nv = float(s)\n        if nv == 1.0: return 1\n        if nv == 0.0: return 0\n    except:\n        pass\n    return np.nan\n\nraw_labels = df[label_col]\ny_true = raw_labels.map(map_label_val)\n\n# drop unknowns\nif y_true.isnull().any():\n    drop_count = int(y_true.isnull().sum())\n    print(f\"\u26a0\ufe0f Dropping {drop_count} rows with unmappable labels.\")\n    valid_mask = ~y_true.isnull()\n    df = df.loc[valid_mask].reset_index(drop=True)\n    y_true = y_true.loc[valid_mask].reset_index(drop=True)\nelse:\n    y_true = y_true.reset_index(drop=True)\n\ny_true = y_true.astype(int)\nprint(\"Final eval rows:\", len(y_true))\nif len(np.unique(y_true)) < 2:\n    raise RuntimeError(\"Need at least two label classes in test set for evaluation.\")\n\n# -------------- evaluate each benchmark --------------\nresults = []\nconfusion_matrices = {}\n\nfor tool in benchmarks:\n    if tool not in df.columns:\n        print(f\"\u26a0\ufe0f Skipping {tool} (not present in parquet).\")\n        continue\n\n    scores = df[tool].astype(float).copy()\n    # fill NaN with median (stable)\n    if scores.isna().any():\n        median_val = float(scores.median())\n        scores = scores.fillna(median_val)\n\n    # handle constant scores (no thresholds)\n    unique_vals = np.unique(scores)\n    if len(unique_vals) == 1:\n        # fallback threshold = median (or 0.5) and compute metrics\n        best_thresh = unique_vals[0]\n        y_pred = (scores >= best_thresh).astype(int)\n        acc = accuracy_score(y_true, y_pred)\n        f1_best = f1_score(y_true, y_pred, zero_division=0)\n        results.append({\n            \"Model\": tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\"),\n            \"Accuracy\": acc,\n            \"Best_Threshold\": float(best_thresh),\n            \"F1_Score\": float(f1_best),\n            \"AP\": float(average_precision_score(y_true, scores))\n        })\n        confusion_matrices[tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\")] = confusion_matrix(y_true, y_pred)\n        print(f\"   {tool}: constant scores \u2014 using threshold {best_thresh:.4f}\")\n        continue\n\n    # compute PR curve then choose threshold maximizing F1 (properly index thresholds)\n    precision, recall, thresholds = precision_recall_curve(y_true, scores)\n    # precision,recall length = len(thresholds)+1\n    f1_all = (2 * precision * recall) / (precision + recall + 1e-12)\n    # only consider indices that have associated thresholds => 1..end\n    if len(thresholds) > 0:\n        idx_rel = np.argmax(f1_all[1:])  # best among entries that map to thresholds\n        best_idx = idx_rel + 1           # absolute index into precision/recall arrays\n        best_thresh = float(thresholds[best_idx - 1])\n    else:\n        # very rare: fallback\n        best_thresh = float(np.median(scores))\n\n    y_pred = (scores >= best_thresh).astype(int)\n    acc = accuracy_score(y_true, y_pred)\n    f1_best = f1_score(y_true, y_pred, zero_division=0)\n    ap = average_precision_score(y_true, scores)\n\n    results.append({\n        \"Model\": tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\"),\n        \"Accuracy\": acc,\n        \"Best_Threshold\": best_thresh,\n        \"F1_Score\": float(f1_best),\n        \"AP\": float(ap)\n    })\n    confusion_matrices[tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\")] = confusion_matrix(y_true, y_pred)\n\n    print(f\"   {tool}: Acc={acc:.4f} F1={f1_best:.4f} AP={ap:.4f} thresh={best_thresh:.4f}\")\n\n# -------------- report --------------\nif not results:\n    print(\"\u274c No benchmarks found.\")\nelse:\n    res_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83c\udfc6 BENCHMARK LEADERBOARD (thresholds optimized for F1)\")\n    print(\"=\" * 60)\n    display_cols = [\"Model\", \"Accuracy\", \"Best_Threshold\", \"F1_Score\", \"AP\"]\n    print(res_df[display_cols].to_string(index=False, formatters={\n        'Accuracy': '{:.2%}'.format,\n        'Best_Threshold': '{:.4f}'.format,\n        'F1_Score': '{:.4f}'.format,\n        'AP': '{:.4f}'.format\n    }))\n\n    # plot confusion matrices in grid\n    n_tools = len(res_df)\n    cols = 3\n    rows = (n_tools + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))\n    axes = axes.flatten()\n\n    for i, row in res_df.iterrows():\n        name = row[\"Model\"]\n        cm = confusion_matrices.get(name)\n        ax = axes[i]\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax,\n                    xticklabels=[\"Pred Benign\", \"Pred Patho\"], yticklabels=[\"Act Benign\", \"Act Patho\"])\n        ax.set_title(f\"{name}\\nAcc: {row['Accuracy']:.1%}  F1: {row['F1_Score']:.3f}\")\n\n    # hide unused axes\n    for j in range(i + 1, len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n",
    "outputs": [
      {
        "type": "stream",
        "text": "Loaded test rows: 300000\nFinal eval rows: 300000\n   CADD_raw_rankscore: Acc=0.4964 F1=0.6372 AP=0.8636 thresh=0.6457\n   REVEL_rankscore: Acc=0.5144 F1=0.6622 AP=0.6449 thresh=0.4644\n   AlphaMissense_rankscore: Acc=0.5152 F1=0.6620 AP=0.6435 thresh=0.2841\n   PrimateAI_rankscore: Acc=0.5042 F1=0.6573 AP=0.6171 thresh=0.2384\n   MPC_rankscore: Acc=0.4952 F1=0.6519 AP=0.6013 thresh=0.2343\n   ClinPred_rankscore: Acc=0.5199 F1=0.6661 AP=0.6644 thresh=0.1142\n   BayesDel_addAF_rankscore: Acc=0.5170 F1=0.6560 AP=0.8867 thresh=0.6330\n   EVE_rankscore: Acc=0.4831 F1=0.6496 AP=0.5674 thresh=0.0924\n   ESM1b_rankscore: Acc=0.5072 F1=0.6565 AP=0.6269 thresh=0.3914\n\n============================================================\n\ud83c\udfc6 BENCHMARK LEADERBOARD (thresholds optimized for F1)\n============================================================\n         Model Accuracy Best_Threshold F1_Score     AP\n      ClinPred   51.99%         0.1142   0.6661 0.6644\nBayesDel_addAF   51.70%         0.6330   0.6560 0.8867\n AlphaMissense   51.52%         0.2841   0.6620 0.6435\n         REVEL   51.44%         0.4644   0.6622 0.6449\n         ESM1b   50.72%         0.3914   0.6565 0.6269\n     PrimateAI   50.42%         0.2384   0.6573 0.6171\n          CADD   49.64%         0.6457   0.6372 0.8636\n           MPC   49.52%         0.2343   0.6519 0.6013\n           EVE   48.31%         0.0924   0.6496 0.5674\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "#same test on train dataset\n# ============================================================\n# CELL: BENCHMARK ACCURACY & CONFUSION MATRIX (robust)\n# ============================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_recall_curve,\n    f1_score,\n    average_precision_score,\n)\n\n# -------------- config --------------\nTEST_FILE = \"train_enriched.parquet\"\n\nbenchmarks = [\n    \"CADD_raw_rankscore\", \"REVEL_rankscore\", \"AlphaMissense_rankscore\",\n    \"PrimateAI_rankscore\", \"MPC_rankscore\", \"ClinPred_rankscore\",\n    \"BayesDel_addAF_rankscore\", \"EVE_rankscore\", \"ESM1b_rankscore\"\n]\n\n# -------------- load --------------\ndf = pd.read_parquet(TEST_FILE)\nprint(\"Loaded test rows:\", len(df))\n\n# -------------- clean & map labels --------------\n# find label column\nlabel_col = None\nfor c in (\"clean_label\", \"label\", \"labels\"):\n    if c in df.columns:\n        label_col = c\n        break\nif label_col is None:\n    raise KeyError(\"Could not find 'label', 'clean_label', or 'labels' column in test file.\")\n\ndef map_label_val(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower()\n    if s in (\"pathogenic\", \"path\", \"p\", \"1\", \"true\", \"t\", \"yes\", \"y\"):\n        return 1\n    if s in (\"benign\", \"benign \", \"b\", \"0\", \"false\", \"f\", \"no\", \"n\"):\n        return 0\n    # try numeric\n    try:\n        nv = float(s)\n        if nv == 1.0: return 1\n        if nv == 0.0: return 0\n    except:\n        pass\n    return np.nan\n\nraw_labels = df[label_col]\ny_true = raw_labels.map(map_label_val)\n\n# drop unknowns\nif y_true.isnull().any():\n    drop_count = int(y_true.isnull().sum())\n    print(f\"\u26a0\ufe0f Dropping {drop_count} rows with unmappable labels.\")\n    valid_mask = ~y_true.isnull()\n    df = df.loc[valid_mask].reset_index(drop=True)\n    y_true = y_true.loc[valid_mask].reset_index(drop=True)\nelse:\n    y_true = y_true.reset_index(drop=True)\n\ny_true = y_true.astype(int)\nprint(\"Final eval rows:\", len(y_true))\nif len(np.unique(y_true)) < 2:\n    raise RuntimeError(\"Need at least two label classes in test set for evaluation.\")\n\n# -------------- evaluate each benchmark --------------\nresults = []\nconfusion_matrices = {}\n\nfor tool in benchmarks:\n    if tool not in df.columns:\n        print(f\"\u26a0\ufe0f Skipping {tool} (not present in parquet).\")\n        continue\n\n    scores = df[tool].astype(float).copy()\n    # fill NaN with median (stable)\n    if scores.isna().any():\n        median_val = float(scores.median())\n        scores = scores.fillna(median_val)\n\n    # handle constant scores (no thresholds)\n    unique_vals = np.unique(scores)\n    if len(unique_vals) == 1:\n        # fallback threshold = median (or 0.5) and compute metrics\n        best_thresh = unique_vals[0]\n        y_pred = (scores >= best_thresh).astype(int)\n        acc = accuracy_score(y_true, y_pred)\n        f1_best = f1_score(y_true, y_pred, zero_division=0)\n        results.append({\n            \"Model\": tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\"),\n            \"Accuracy\": acc,\n            \"Best_Threshold\": float(best_thresh),\n            \"F1_Score\": float(f1_best),\n            \"AP\": float(average_precision_score(y_true, scores))\n        })\n        confusion_matrices[tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\")] = confusion_matrix(y_true, y_pred)\n        print(f\"   {tool}: constant scores \u2014 using threshold {best_thresh:.4f}\")\n        continue\n\n    # compute PR curve then choose threshold maximizing F1 (properly index thresholds)\n    precision, recall, thresholds = precision_recall_curve(y_true, scores)\n    # precision,recall length = len(thresholds)+1\n    f1_all = (2 * precision * recall) / (precision + recall + 1e-12)\n    # only consider indices that have associated thresholds => 1..end\n    if len(thresholds) > 0:\n        idx_rel = np.argmax(f1_all[1:])  # best among entries that map to thresholds\n        best_idx = idx_rel + 1           # absolute index into precision/recall arrays\n        best_thresh = float(thresholds[best_idx - 1])\n    else:\n        # very rare: fallback\n        best_thresh = float(np.median(scores))\n\n    y_pred = (scores >= best_thresh).astype(int)\n    acc = accuracy_score(y_true, y_pred)\n    f1_best = f1_score(y_true, y_pred, zero_division=0)\n    ap = average_precision_score(y_true, scores)\n\n    results.append({\n        \"Model\": tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\"),\n        \"Accuracy\": acc,\n        \"Best_Threshold\": best_thresh,\n        \"F1_Score\": float(f1_best),\n        \"AP\": float(ap)\n    })\n    confusion_matrices[tool.replace(\"_rankscore\", \"\").replace(\"_raw\", \"\")] = confusion_matrix(y_true, y_pred)\n\n    print(f\"   {tool}: Acc={acc:.4f} F1={f1_best:.4f} AP={ap:.4f} thresh={best_thresh:.4f}\")\n\n# -------------- report --------------\nif not results:\n    print(\"\u274c No benchmarks found.\")\nelse:\n    res_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83c\udfc6 BENCHMARK LEADERBOARD (thresholds optimized for F1)\")\n    print(\"=\" * 60)\n    display_cols = [\"Model\", \"Accuracy\", \"Best_Threshold\", \"F1_Score\", \"AP\"]\n    print(res_df[display_cols].to_string(index=False, formatters={\n        'Accuracy': '{:.2%}'.format,\n        'Best_Threshold': '{:.4f}'.format,\n        'F1_Score': '{:.4f}'.format,\n        'AP': '{:.4f}'.format\n    }))\n\n    # plot confusion matrices in grid\n    n_tools = len(res_df)\n    cols = 3\n    rows = (n_tools + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))\n    axes = axes.flatten()\n\n    for i, row in res_df.iterrows():\n        name = row[\"Model\"]\n        cm = confusion_matrices.get(name)\n        ax = axes[i]\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax,\n                    xticklabels=[\"Pred Benign\", \"Pred Patho\"], yticklabels=[\"Act Benign\", \"Act Patho\"])\n        ax.set_title(f\"{name}\\nAcc: {row['Accuracy']:.1%}  F1: {row['F1_Score']:.3f}\")\n\n    # hide unused axes\n    for j in range(i + 1, len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n",
    "outputs": [
      {
        "type": "stream",
        "text": "Loaded test rows: 1200890\nFinal eval rows: 1200890\n   CADD_raw_rankscore: Acc=0.2024 F1=0.2184 AP=0.7528 thresh=0.6457\n   REVEL_rankscore: Acc=0.9008 F1=0.4104 AP=0.3705 thresh=0.6705\n   AlphaMissense_rankscore: Acc=0.8988 F1=0.4034 AP=0.3643 thresh=0.5102\n   PrimateAI_rankscore: Acc=0.8826 F1=0.3624 AP=0.2936 thresh=0.3776\n   MPC_rankscore: Acc=0.8811 F1=0.3335 AP=0.2826 thresh=0.3977\n   ClinPred_rankscore: Acc=0.9094 F1=0.4621 AP=0.4051 thresh=0.3591\n   BayesDel_addAF_rankscore: Acc=0.9670 F1=0.8504 AP=0.7997 thresh=0.6334\n   EVE_rankscore: Acc=0.8960 F1=0.2945 AP=0.2636 thresh=0.5810\n   ESM1b_rankscore: Acc=0.8939 F1=0.3758 AP=0.3339 thresh=0.5267\n\n============================================================\n\ud83c\udfc6 BENCHMARK LEADERBOARD (thresholds optimized for F1)\n============================================================\n         Model Accuracy Best_Threshold F1_Score     AP\nBayesDel_addAF   96.70%         0.6334   0.8504 0.7997\n      ClinPred   90.94%         0.3591   0.4621 0.4051\n         REVEL   90.08%         0.6705   0.4104 0.3705\n AlphaMissense   89.88%         0.5102   0.4034 0.3643\n           EVE   89.60%         0.5810   0.2945 0.2636\n         ESM1b   89.39%         0.5267   0.3758 0.3339\n     PrimateAI   88.26%         0.3776   0.3624 0.2936\n           MPC   88.11%         0.3977   0.3335 0.2826\n          CADD   20.24%         0.6457   0.2184 0.7528\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ==========================================\n# \ud83d\udcca CELL 5: SHAP EXPLAINABILITY (DNA vs Tabular) \u2014 FIXED\n# ==========================================\n!pip install -q shap matplotlib seaborn\n\nimport shap\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nprint(\"\ud83e\udde0 INITIALIZING SHAP ANALYSIS...\")\n\n# 1. CONFIGURATION\n# SHAP is expensive. We use a representative subset.\nN_BACKGROUND = 100\nN_TEST = 50\nTOTAL_SAMPLES = N_BACKGROUND + N_TEST\n\n# 2. DEFINE SHAP WRAPPER (Focus on Classifier Head)\nclass HeadWrapper(torch.nn.Module):\n    def __init__(self, classifier):\n        super().__init__()\n        self.classifier = classifier\n\n    def forward(self, combined_input):\n        return self.classifier(combined_input)\n\n# Handle compiled models\nraw_model = model if not hasattr(model, \"_orig_mod\") else model._orig_mod\nshap_model = HeadWrapper(raw_model.classifier).to(\"cpu\").eval()\n\n# 3. GENERATE LATENT EMBEDDINGS\nprint(f\"\ud83d\udcc9 Extracting embeddings for {TOTAL_SAMPLES} samples...\")\n\ndataloader = torch.utils.data.DataLoader(\n    test_fast, \n    batch_size=32, \n    shuffle=True, \n    collate_fn=data_collator\n)\n\nlatent_vectors = []\ncollected = 0\n\nmodel.eval()\n\n# --- FIX IS HERE: Get device from the first parameter ---\ndevice = next(model.parameters()).device \n# --------------------------------------------------------\n\nfor batch in tqdm(dataloader, desc=\"Encoding Data\"):\n    if collected >= TOTAL_SAMPLES: break\n    \n    with torch.no_grad():\n        input_ids = batch[\"input_ids\"].to(device)\n        mask = batch[\"attention_mask\"].to(device)\n        # Ensure features are float32 before moving to device\n        feats = batch[\"features\"].to(dtype=torch.float32).to(device)\n        \n        # A. Get DNA Embedding\n        enc_module = raw_model.encoder\n        out = enc_module(input_ids=input_ids, attention_mask=mask, return_dict=True)\n        \n        # Mean Pooling\n        if hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n            pooled = out.pooler_output\n        else:\n            hidden = out.last_hidden_state\n            mask_exp = mask.unsqueeze(-1)\n            pooled = (hidden * mask_exp).sum(1) / mask_exp.sum(1).clamp(min=1e-9)\n            \n        # B. Concatenate\n        combined = torch.cat([pooled, feats], dim=1).cpu()\n        latent_vectors.append(combined)\n        collected += len(batch[\"input_ids\"])\n\n# Prepare Tensors\nfull_data = torch.cat(latent_vectors, dim=0)\nbackground_data = full_data[:N_BACKGROUND]\ntest_data = full_data[N_BACKGROUND : N_BACKGROUND+N_TEST]\n\nprint(f\"\u2705 Data Prepared. Background shape: {background_data.shape}\")\n\n# 4. RUN SHAP (GradientExplainer)\nprint(\"\u26a1 Calculating SHAP values (GradientExplainer)...\")\nexplainer = shap.GradientExplainer(shap_model, background_data)\nshap_values = explainer.shap_values(test_data)\n\n# Handle list output\nif isinstance(shap_values, list):\n    vals = shap_values[1]\nelse:\n    vals = shap_values\n\nprint(\"\u2705 SHAP Calculation Complete.\")\n\n# ==========================================\n# \ud83d\udcc8 PLOTTING\n# ==========================================\n\n# A. TABULAR FEATURE IMPORTANCE\n# We look only at the last columns corresponding to features\ntabular_shap = vals[:, -len(feature_cols):]\ntabular_data = test_data[:, -len(feature_cols):].numpy()\n\nplt.figure(figsize=(10, 6))\nplt.title(\"Impact of Clinical Features on Prediction\")\nshap.summary_plot(\n    tabular_shap, \n    tabular_data, \n    feature_names=feature_cols, \n    show=False,\n    plot_type=\"dot\"\n)\nplt.tight_layout()\nplt.savefig(\"shap_tabular_beeswarm.png\", dpi=300)\nplt.show()\n\n# B. DNA vs TABULAR BATTLE\ndna_impact = np.abs(vals[:, :-len(feature_cols)]).sum(axis=1).mean()\ntab_impact = np.abs(vals[:, -len(feature_cols):]).sum(axis=1).mean()\n\ntotal = dna_impact + tab_impact\ndna_pct = (dna_impact / total) * 100\ntab_pct = (tab_impact / total) * 100\n\nplt.figure(figsize=(8, 5))\nsns.barplot(\n    x=[\"DNA Sequence\", \"Clinical Features\"], \n    y=[dna_impact, tab_impact], \n    palette=[\"#2ecc71\", \"#3498db\"]\n)\nplt.title(f\"Model Intelligence Source\\n(DNA: {dna_pct:.1f}% vs Features: {tab_pct:.1f}%)\")\nplt.ylabel(\"Mean Absolute SHAP Value\")\nplt.savefig(\"shap_modality_comparison.png\", dpi=300)\nplt.show()\n\nprint(\"\\n\ud83c\udfc6 FINAL VERDICT:\")\nif dna_pct > 50:\n    print(f\"\u2705 SUPERHUMAN: The model is primarily looking at the DNA ({dna_pct:.1f}%)!\")\nelse:\n    print(f\"\u2696\ufe0f HYBRID: The model relies heavily on conservation scores ({tab_pct:.1f}%)!\")",
    "outputs": [
      {
        "type": "stream",
        "text": "\ud83e\udde0 INITIALIZING SHAP ANALYSIS...\n\ud83d\udcc9 Extracting embeddings for 150 samples...\n"
      },
      {
        "type": "stream",
        "text": "Encoding Data:   1%|          | 5/440 [00:03<04:30,  1.61it/s]\n"
      },
      {
        "type": "stream",
        "text": "\u2705 Data Prepared. Background shape: torch.Size([100, 1289])\n\u26a1 Calculating SHAP values (GradientExplainer)...\n\u2705 SHAP Calculation Complete.\n"
      },
      {
        "type": "stream",
        "text": "/tmp/ipykernel_10337/4129238603.py:111: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n  shap.summary_plot(\n/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/shap/plots/_beeswarm.py:723: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n  summary_legacy(\n/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/shap/plots/_beeswarm.py:743: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n  summary_legacy(\n"
      },
      {
        "type": "stream",
        "text": "/tmp/ipykernel_10337/4129238603.py:131: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n"
      },
      {
        "type": "stream",
        "text": "\n\ud83c\udfc6 FINAL VERDICT:\n\u2705 SUPERHUMAN: The model is primarily looking at the DNA (64.9%)!\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ======================================================\n# \ud83d\udd0c Bridge: DataFrame \u2192 HF-style Torch batch for Ablation\n# ======================================================\n\ndef prepare_batch(df_batch):\n    # 1) Tokenize DNA\n    enc = tokenizer(\n        df_batch[\"raw_sequence\"].tolist(),\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_LEN,\n        return_tensors=\"pt\"\n    )\n\n    # 2) Pack features in correct order\n    feats = []\n    for c in feature_cols:\n        feats.append(df_batch[c].astype(np.float32).values)\n\n    feats = np.stack(feats, axis=1)  # (batch, n_features)\n\n    # 3) Labels\n    labels = (df_batch[\"clean_label\"] == \"Pathogenic\").astype(np.int64).values\n\n    return {\n        \"input_ids\": enc[\"input_ids\"],\n        \"attention_mask\": enc[\"attention_mask\"],\n        \"features\": torch.tensor(feats, dtype=torch.float32),\n        \"labels\": torch.tensor(labels, dtype=torch.long),\n    }\n",
    "outputs": []
  },
  {
    "type": "code",
    "source": "# ==========================================\n# \ud83d\udcc9 CELL 2: FULL ABLATION STUDY (Tensor-Native) \u2014 FIXED\n# ==========================================\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_curve, auc\nfrom torch.utils.data import DataLoader\n\nprint(\"\ud83e\uddea STARTING FULL ABLATION SUITE (Tensor-Native)...\")\n\n# 1. SETUP DEVICE & MODEL\n# Explicitly force GPU to fix \"Device Mismatch\" error\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(DEVICE)\nprint(f\"\ud83d\udd25 Model locked to: {DEVICE}\")\n\n# 2. DEFINE ABLATION FUNCTION\ndef evaluate_tensor_ablation(test_config, baseline_auc=None):\n    removals = test_config.get(\"remove\", [])\n    name = test_config[\"name\"]\n    \n    all_preds = []\n    all_labels = []\n    BATCH_SIZE = 128\n    \n    # Create DataLoader from 'test_fast'\n    loader = DataLoader(test_fast, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)\n    \n    model.eval()\n    \n    for batch in tqdm(loader, desc=f\"Testing: {name}\", leave=False):\n        # Move inputs to GPU\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        mask = batch[\"attention_mask\"].to(DEVICE)\n        features = batch[\"features\"].to(DEVICE)\n        labels = batch[\"labels\"].to(\"cpu\") # Keep labels on CPU for sklearn metrics\n        \n        # --- DESTRUCTION LOGIC ---\n        \n        # 1. Kill DNA? (Shuffle input_ids within batch)\n        if \"DNA\" in removals:\n            perm = torch.randperm(input_ids.size(0))\n            input_ids = input_ids[perm]\n            mask = mask[perm]\n            \n        # 2. Kill Tabular Features? (Zero out columns)\n        if \"gnomAD\" in removals:\n            indices = [i for i, c in enumerate(feature_cols) if \"gnomad\" in c.lower()]\n            for idx in indices:\n                features[:, idx] = 0.0\n                \n        if \"GERP\" in removals:\n            indices = [i for i, c in enumerate(feature_cols) if \"gerp\" in c.lower()]\n            for idx in indices:\n                features[:, idx] = 0.0\n\n        if \"PhyloP\" in removals:\n            indices = [i for i, c in enumerate(feature_cols) if \"phylop\" in c.lower()]\n            for idx in indices:\n                features[:, idx] = 0.0\n                \n        if \"PhastCons\" in removals:\n            indices = [i for i, c in enumerate(feature_cols) if \"phastcons\" in c.lower()]\n            for idx in indices:\n                features[:, idx] = 0.0\n                \n        if \"All_Conservation\" in removals:\n             gnomad_idxs = [i for i, c in enumerate(feature_cols) if \"gnomad\" in c.lower()]\n             # Kill everything that is NOT gnomAD\n             for i in range(features.size(1)):\n                 if i not in gnomad_idxs:\n                     features[:, i] = 0.0\n        \n        if \"All_Tabular\" in removals:\n             features.fill_(0.0)\n\n        # --- INFERENCE ---\n        with torch.no_grad():\n            # Ensure float32 for features\n            features = features.to(dtype=torch.float32)\n            \n            out = model(input_ids=input_ids, attention_mask=mask, features=features)\n            logits = out.logits\n            probs = torch.softmax(logits, dim=1)[:, 1]\n            \n            all_preds.extend(probs.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            \n    # Metrics\n    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n    roc_auc = auc(fpr, tpr)\n    \n    drop = 0.0\n    if baseline_auc is not None:\n        drop = baseline_auc - roc_auc\n    \n    return {\"name\": name, \"auc\": roc_auc, \"drop\": drop, \"fpr\": fpr, \"tpr\": tpr}\n\n# 3. CALCULATE BASELINE\nprint(\"\ud83d\udcca Establishing Baseline Performance...\")\nbaseline_res = evaluate_tensor_ablation({\"name\": \"Baseline Model\", \"remove\": []})\nbaseline_auc = baseline_res[\"auc\"]\nprint(f\"\u2705 Baseline AUC established: {baseline_auc:.4f}\")\n\n# 4. DEFINE ABLATION GROUPS\nablation_tests = [\n    {\"name\": \"No DNA (Sequence Blind)\",       \"remove\": [\"DNA\"]},\n    {\"name\": \"No gnomAD (Freq Blind)\",        \"remove\": [\"gnomAD\"]},\n    {\"name\": \"No GERP (Evo Blind)\",           \"remove\": [\"GERP\"]},\n    {\"name\": \"No PhyloP\",                     \"remove\": [\"PhyloP\"]},\n    {\"name\": \"No PhastCons\",                  \"remove\": [\"PhastCons\"]},\n    {\"name\": \"No Conservation (All Scores)\",  \"remove\": [\"All_Conservation\"]},\n    {\"name\": \"No DNA + No gnomAD\",            \"remove\": [\"DNA\", \"gnomAD\"]},\n    {\"name\": \"No DNA + No Conservation\",      \"remove\": [\"DNA\", \"All_Conservation\"]},\n    {\"name\": \"No gnomAD + No Conservation\",   \"remove\": [\"gnomAD\", \"All_Conservation\"]},\n    {\"name\": \"No Tabular (Pure DNA)\",         \"remove\": [\"All_Tabular\"]}\n]\n\n# 5. RUN ALL TESTS\nresults = [baseline_res] \nplt.figure(figsize=(12, 8))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\n# Plot Baseline\nplt.plot(baseline_res[\"fpr\"], baseline_res[\"tpr\"], lw=3, color='black', \n         label=f'Baseline (AUC={baseline_auc:.3f})')\n\nprint(\"\\n\ud83d\udd25 RUNNING TENSOR ABLATION TESTS...\")\nfor test in ablation_tests:\n    res = evaluate_tensor_ablation(test, baseline_auc=baseline_auc)\n    results.append(res)\n    \n    plt.plot(res[\"fpr\"], res[\"tpr\"], lw=1.5, alpha=0.7, label=f'{res[\"name\"]} (AUC={res[\"auc\"]:.3f})')\n\n# 6. FINALIZE PLOT\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('PathoPreter Ablation: Feature Importance Analysis')\nplt.legend(loc=\"lower right\", fontsize='small')\nplt.grid(True, alpha=0.3)\nplt.savefig(\"ablation_study_10_tests.png\", dpi=300)\nplt.show()\n\n# 7. PRINT LEADERBOARD\nprint(\"\\n\ud83c\udfc6 ABLATION IMPACT RANKING (What matters most?):\")\nres_df = pd.DataFrame([r for r in results if r[\"name\"] != \"Baseline Model\"])\nres_df = res_df[[\"name\", \"auc\", \"drop\"]]\nres_df = res_df.sort_values(\"drop\", ascending=False)\nprint(res_df.to_string(index=False))",
    "outputs": [
      {
        "type": "stream",
        "text": "\ud83e\uddea STARTING FULL ABLATION SUITE (Tensor-Native)...\n\ud83d\udd25 Model locked to: cuda\n\ud83d\udcca Establishing Baseline Performance...\n"
      },
      {
        "type": "stream",
        "text": "Testing: Baseline Model:   1%|          | 1/110 [00:01<02:15,  1.25s/it]"
      },
      {
        "type": "stream",
        "text": "                                                                          \r"
      },
      {
        "type": "stream",
        "text": "\u2705 Baseline AUC established: 0.9182\n\n\ud83d\udd25 RUNNING TENSOR ABLATION TESTS...\n"
      },
      {
        "type": "stream",
        "text": "                                                                                        \r"
      },
      {
        "type": "stream",
        "text": "\n\ud83c\udfc6 ABLATION IMPACT RANKING (What matters most?):\n                        name      auc      drop\n    No DNA + No Conservation 0.517988  0.400191\n          No DNA + No gnomAD 0.557964  0.360215\n     No DNA (Sequence Blind) 0.558319  0.359860\n                No PhastCons 0.901000  0.017178\n       No Tabular (Pure DNA) 0.908187  0.009992\n No gnomAD + No Conservation 0.908187  0.009992\nNo Conservation (All Scores) 0.911717  0.006462\n      No gnomAD (Freq Blind) 0.915300  0.002879\n                   No PhyloP 0.917488  0.000691\n         No GERP (Evo Blind) 0.922640 -0.004461\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ==========================================\n# \ud83d\udce6 CELL 6: THE \"TIME CAPSULE\" PACKAGER\n# ==========================================\nimport json\nimport zipfile\nimport os\nfrom tqdm import tqdm\n\n# Configuration\nSOURCE_DIR = \"./PathoPreter_Final_A100\"\nOUTPUT_ZIP = \"PathoPreter_A100_Complete.zip\"\n\nprint(f\"\ud83d\udce6 PACKAGING MODEL FROM: {SOURCE_DIR}\")\n\n# 1. SAVE CRITICAL METADATA (Column Names)\n# This is the #1 thing people forget 2 months later!\nmetadata = {\n    \"feature_cols\": feature_cols,\n    \"num_labels\": NUM_LABELS,\n    \"feature_dim\": FEATURE_DIM,\n    \"model_type\": \"nt-500m-hybrid\",\n    \"base_model\": \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n}\n\nwith open(os.path.join(SOURCE_DIR, \"model_metadata.json\"), \"w\") as f:\n    json.dump(metadata, f, indent=4)\nprint(\"\u2705 Saved model_metadata.json (feature columns preserved)\")\n\n# 2. GENERATE PYTHON MODEL CLASS FILE\n# We write the class definition to a .py file so you can import it easily later\nmodel_code = \"\"\"\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nclass SequenceClassificationWithFeatures(nn.Module):\n    def __init__(self, encoder, hidden_size, feature_dim, num_labels):\n        super().__init__()\n        self.encoder = encoder\n        self.feature_dim = feature_dim\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size + feature_dim, hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, num_labels)\n        )\n\n    @classmethod\n    def from_pretrained(cls, path, device=\"cpu\"):\n        import json\n        import os\n        from safetensors.torch import load_file\n        \n        # Load Config\n        with open(os.path.join(path, \"model_metadata.json\"), \"r\") as f:\n            meta = json.load(f)\n            \n        # Load Encoder\n        encoder = AutoModel.from_pretrained(path, trust_remote_code=True)\n        \n        # Init Model\n        model = cls(\n            encoder=encoder, \n            hidden_size=encoder.config.hidden_size,\n            feature_dim=meta[\"feature_dim\"],\n            num_labels=meta[\"num_labels\"]\n        )\n        \n        # Load Weights\n        state_dict = load_file(os.path.join(path, \"model.safetensors\"))\n        model.load_state_dict(state_dict)\n        \n        return model.to(device)\n\n    def forward(self, input_ids=None, attention_mask=None, features=None, labels=None):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n\n        if hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n            pooled = out.pooler_output\n        else:\n            last = out.last_hidden_state\n            mask = attention_mask.unsqueeze(-1)\n            pooled = (last * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n\n        # Cast features to match model dtype/device\n        features = features.to(pooled.device).to(pooled.dtype)\n        x = torch.cat([pooled, features], dim=1)\n        logits = self.classifier(x)\n\n        loss = None\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n\n        return SequenceClassifierOutput(loss=loss, logits=logits)\n\"\"\"\n\nwith open(os.path.join(SOURCE_DIR, \"modeling_pathopreter.py\"), \"w\") as f:\n    f.write(model_code)\nprint(\"\u2705 Generated modeling_pathopreter.py (Auto-load script)\")\n\n# 3. CREATE README INSTRUCTIONS\nreadme_text = \"\"\"\nPATHO-PRETER v1 (A100 Fine-Tune)\n================================\nTrained by: Rohit\nBase Model: Nucleotide Transformer 500M\nArchitecture: Hybrid (DNA Sequence + 9 Conservation Scores)\n\nHOW TO LOAD:\n1. Unzip this folder.\n2. Place 'modeling_pathopreter.py' in your python path.\n3. Run:\n   \n   from modeling_pathopreter import SequenceClassificationWithFeatures\n   import numpy as np\n\n   # Load Model\n   model = SequenceClassificationWithFeatures.from_pretrained(\"./PathoPreter_Final_A100\")\n   \n   # Load Scalers (Crucial for inference!)\n   means = np.load(\"./PathoPreter_Final_A100/feat_means.npy\")\n   stds = np.load(\"./PathoPreter_Final_A100/feat_stds.npy\")\n\n   # Preprocessing\n   # (features - means) / stds\n\"\"\"\nwith open(os.path.join(SOURCE_DIR, \"README.txt\"), \"w\") as f:\n    f.write(readme_text)\n\n# 4. SMART ZIP (No Compression for Weights)\nprint(f\"\ud83d\ude80 Zipping to {OUTPUT_ZIP}...\")\n\nwith zipfile.ZipFile(OUTPUT_ZIP, 'w') as zf:\n    for root, dirs, files in os.walk(SOURCE_DIR):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, start=SOURCE_DIR)\n            \n            # DECISION: Compress text, Store binary weights (for speed)\n            if file.endswith(\".safetensors\") or file.endswith(\".bin\"):\n                compression = zipfile.ZIP_STORED\n                print(f\"  - Storing (Fast): {arcname}\")\n            else:\n                compression = zipfile.ZIP_DEFLATED\n                \n            zf.write(file_path, arcname, compress_type=compression)\n\nprint(f\"\\n\u2705 SUCCESS! Download '{OUTPUT_ZIP}' now.\")\nprint(\"   It contains the Model, Tokenizer, Scalers, and Python Class Code.\")",
    "outputs": [
      {
        "type": "stream",
        "text": "\ud83d\udce6 PACKAGING MODEL FROM: ./PathoPreter_Final_A100\n\u2705 Saved model_metadata.json (feature columns preserved)\n\u2705 Generated modeling_pathopreter.py (Auto-load script)\n\ud83d\ude80 Zipping to PathoPreter_A100_Complete.zip...\n  - Storing (Fast): model.safetensors\n\n\u2705 SUCCESS! Download 'PathoPreter_A100_Complete.zip' now.\n   It contains the Model, Tokenizer, Scalers, and Python Class Code.\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ==============================\n# Rare vs Common Variant Audit\n# ==============================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\n\ndf = pd.read_parquet(\"test_enriched_SEQUENCES.parquet\").reset_index(drop=True)\ndf[\"PathoPreter\"] = probs\ndf[\"y_true\"] = y_true\n\n# Define frequency bins\ndf[\"af_bin\"] = pd.cut(\n    df[\"gnomad_af\"],\n    bins=[0, 1e-6, 1e-4, 1e-2, 1],\n    labels=[\"Ultra-rare\", \"Rare\", \"Low-freq\", \"Common\"]\n)\n\nprint(\"\\n\ud83e\uddec Performance by Allele Frequency:\")\nfor bin in df[\"af_bin\"].dropna().unique():\n    sub = df[df[\"af_bin\"] == bin]\n    if len(sub[\"y_true\"].unique()) > 1:\n        auc = roc_auc_score(sub[\"y_true\"], sub[\"PathoPreter\"])\n        print(f\"{bin:10s} | N={len(sub):6d} | AUC={auc:.4f}\")\n",
    "outputs": [
      {
        "type": "stream",
        "text": "\n\ud83e\uddec Performance by Allele Frequency:\nLow-freq   | N=   967 | AUC=0.4675\nUltra-rare | N=  1505 | AUC=0.5082\nRare       | N=  4665 | AUC=0.4973\nCommon     | N=   414 | AUC=0.4309\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "from sklearn.metrics import roc_auc_score\nimport numpy as np\n\n# Use the same objects you created in calibration cell\n# These came from: preds = trainer.predict(test_fast)\n\ny_test = preds.label_ids                 # shape = (N_test,)\nlogits = preds.predictions              # shape = (N_test, 2)\nprobs = torch.softmax(torch.tensor(logits), dim=-1)[:,1].numpy()\n\nprint(\"Samples:\", len(y_test), len(probs))\n\n# --- real AUC ---\nreal_auc = roc_auc_score(y_test, probs)\n\n# --- permutation ---\nperm = np.random.permutation(y_test)\nperm_auc = roc_auc_score(perm, probs)\n\nprint(f\"Real AUC:        {real_auc:.4f}\")\nprint(f\"Permutation AUC:{perm_auc:.4f}\")\n\nif perm_auc < 0.55:\n    print(\"\u2705 No leakage or memorization detected\")\nelse:\n    print(\"\u274c Possible leakage \u2014 investigate splits\")\n",
    "outputs": [
      {
        "type": "stream",
        "text": "Samples: 14073 14073\nReal AUC:        0.9182\nPermutation AUC:0.5044\n\u2705 No leakage or memorization detected\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ======================================================\n# \ud83e\uddec CELL: Performance by Allele Frequency (gnomAD bins)\n# ======================================================\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\n# Reload test dataframe (same order as test_fast)\ndf = pd.read_parquet(\"test_enriched_SEQUENCES.parquet\").reset_index(drop=True)\n\n# Safety check\nassert len(df) == len(probs), \"\u274c Test set mismatch \u2014 check alignment\"\n\n# Attach predictions\ndf[\"PathoPreter\"] = probs\ndf[\"y_true\"] = y_test\n\n# Define frequency bins used in clinical genomics\nbins = [0, 1e-6, 1e-4, 1e-2, 1]\nlabels = [\"Ultra-rare (<1e-6)\", \"Rare (1e-6\u20131e-4)\", \"Low-freq (1e-4\u20131e-2)\", \"Common (>1e-2)\"]\n\ndf[\"AF_bin\"] = pd.cut(df[\"gnomad_af\"], bins=bins, labels=labels, include_lowest=True)\n\nprint(\"\\n\ud83e\uddec PathoPreter Performance by Allele Frequency\")\nprint(\"=\" * 60)\n\nrows = []\nfor bin in labels:\n    sub = df[df[\"AF_bin\"] == bin]\n    if len(sub) < 100:\n        print(f\"{bin:18s} | too few samples ({len(sub)})\")\n        continue\n        \n    if len(np.unique(sub[\"y_true\"])) < 2:\n        print(f\"{bin:18s} | only one class present\")\n        continue\n    \n    auc = roc_auc_score(sub[\"y_true\"], sub[\"PathoPreter\"])\n    pos_rate = sub[\"y_true\"].mean()\n    \n    rows.append((bin, len(sub), pos_rate, auc))\n    print(f\"{bin:18s} | N={len(sub):6d} | Pathogenic%={pos_rate*100:5.1f}% | AUC={auc:.4f}\")\n\n# Summary table\naf_df = pd.DataFrame(rows, columns=[\"AF bin\", \"N\", \"Pathogenic %\", \"ROC-AUC\"])\n\nprint(\"\\n\ud83d\udcca Summary\")\ndisplay(af_df)\n",
    "outputs": [
      {
        "type": "stream",
        "text": "\n\ud83e\uddec PathoPreter Performance by Allele Frequency\n============================================================\nUltra-rare (<1e-6) | N=  8027 | Pathogenic%= 67.1% | AUC=0.9238\nRare (1e-6\u20131e-4)   | N=  4665 | Pathogenic%= 34.7% | AUC=0.9069\nLow-freq (1e-4\u20131e-2) | N=   967 | Pathogenic%=  5.5% | AUC=0.8219\nCommon (>1e-2)     | N=   414 | Pathogenic%=  0.5% | AUC=0.9472\n\n\ud83d\udcca Summary\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "# ==========================================\n# \ud83d\ude80 CELL 7: THE \"SINGLE VARIANT PREDICTOR\" (The App)\n# ==========================================\nimport torch\nimport numpy as np\n\ndef predict_single_variant(dna_sequence, gnomad_af=0.0, conservation_scores=None):\n    \"\"\"\n    Simulates a real-world clinical query.\n    dna_sequence: 60bp string (center is the mutation)\n    gnomad_af: Allele Frequency (0.0 to 1.0)\n    conservation_scores: list of [GERP, PhyloP, etc...] (Defaults to averages if None)\n    \"\"\"\n    \n    # 1. Validation\n    dna_sequence = dna_sequence.upper().strip()\n    if len(dna_sequence) < 10:\n        return \"\u274c Error: Sequence too short. Needs ~60bp context.\"\n    \n    # 2. Tokenize DNA (Using the k-mer tokenizer from training)\n    # Note: We manually simulate tokenization for this demo or use the loaded tokenizer\n    inputs = tokenizer(dna_sequence, return_tensors=\"pt\", padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True)\n    \n    # 3. Prepare Tabular Features\n    # If user doesn't provide detailed scores, we use the DATASET MEANS (Imputation)\n    # This is exactly what web-servers like CADD do for missing data.\n    if conservation_scores is None:\n        # Create a vector of zeros (representing mean values after normalization)\n        # We assume 0.0 because we standardized features (mean=0, std=1) during training\n        feats = np.zeros(len(feature_cols)) \n    else:\n        feats = np.array(conservation_scores)\n\n    # 4. Inject Specific inputs (like gnomAD) if known\n    # Find gnomAD column index\n    try:\n        g_idx = [i for i, c in enumerate(feature_cols) if \"gnomad\" in c.lower()][0]\n        # Normalize gnomAD (Assuming approx mean=0.005, std=0.05 from training data stats)\n        # For the demo, we just set it raw, but in prod you load the scaler.\n        feats[g_idx] = (gnomad_af - feat_means[g_idx]) / feat_stds[g_idx]\n    except:\n        pass\n\n    # 5. Convert to Tensor\n    input_ids = inputs[\"input_ids\"].to(DEVICE)\n    mask = inputs[\"attention_mask\"].to(DEVICE)\n    features_tensor = torch.tensor([feats], dtype=torch.float32).to(DEVICE)\n    \n    # 6. Predict\n    model.eval()\n    with torch.no_grad():\n        out = model(input_ids, mask, features_tensor)\n        prob = torch.softmax(out.logits, dim=1)[0, 1].item()\n        \n    # 7. Format Output\n    risk_level = \"\ud83d\udd34 PATHOGENIC\" if prob > 0.5 else \"\ud83d\udfe2 BENIGN\"\n    confidence = prob if prob > 0.5 else 1 - prob\n    \n    print(\"\\n\" + \"=\"*40)\n    print(f\"\ud83e\uddec VARIANT DIAGNOSIS REPORT\")\n    print(\"=\"*40)\n    print(f\"Input Sequence: {dna_sequence[:10]}...{dna_sequence[-10:]}\")\n    print(f\"Prediction:     {risk_level}\")\n    print(f\"Probability:    {prob:.4f}\")\n    print(f\"Confidence:     {confidence:.1%}\")\n    print(\"=\"*40 + \"\\n\")\n    \n    return prob\n\n# --- TEST CASE 1: A Real Pathogenic Mutation (CFTR DeltaF508 equivalent context) ---\n# High conservation (implicit), low frequency\nseq_patho = \"A\" * 60 # Placeholder sequence for demo\nprint(\"\ud83c\udfe5 CASE 1: Testing potential pathogenic variant...\")\npredict_single_variant(seq_patho, gnomad_af=0.00001)\n\n# --- TEST CASE 2: A Common Benign Variant ---\n# High frequency (50%)\nseq_benign = \"T\" * 60 # Placeholder\nprint(\"\ud83d\ude0a CASE 2: Testing common benign variant...\")\npredict_single_variant(seq_benign, gnomad_af=0.50)",
    "outputs": [
      {
        "type": "stream",
        "text": "\ud83c\udfe5 CASE 1: Testing potential pathogenic variant...\n\n========================================\n\ud83e\uddec VARIANT DIAGNOSIS REPORT\n========================================\nInput Sequence: AAAAAAAAAA...AAAAAAAAAA\nPrediction:     \ud83d\udfe2 BENIGN\nProbability:    0.0124\nConfidence:     98.8%\n========================================\n\n\ud83d\ude0a CASE 2: Testing common benign variant...\n\n========================================\n\ud83e\uddec VARIANT DIAGNOSIS REPORT\n========================================\nInput Sequence: TTTTTTTTTT...TTTTTTTTTT\nPrediction:     \ud83d\udfe2 BENIGN\nProbability:    0.0012\nConfidence:     99.9%\n========================================\n\n"
      },
      {
        "type": "stream",
        "text": "/tmp/ipykernel_10337/1800919678.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n  features_tensor = torch.tensor([feats], dtype=torch.float32).to(DEVICE)\n"
      },
      {
        "type": "result",
        "data": {
          "text/plain": "0.001151399570517242"
        }
      }
    ]
  },
  {
    "type": "code",
    "source": "import torch\nimport numpy as np\n\n# Make sure model is on correct device\nDEVICE = next(model.parameters()).device\n\ndef predict_single_variant(\n    dna_sequence,\n    gnomad_af=0.0,\n    conservation_scores=None,\n):\n    \"\"\"\n    Clinical-style single variant inference.\n\n    dna_sequence: ~60\u20131000bp centered on variant\n    gnomad_af: allele frequency (0\u20131)\n    conservation_scores: list in same order as feature_cols\n                         [GERP++, GERP91, phyloP100, phyloP470, phyloP17,\n                          phastCons100, phastCons470, phastCons17]\n                         (optional)\n    \"\"\"\n\n    # --------------------------\n    # 1. Validate DNA\n    # --------------------------\n    dna_sequence = dna_sequence.upper().strip()\n    if len(dna_sequence) < 30:\n        raise ValueError(\"DNA sequence too short \u2014 need \u226530bp context\")\n\n    # --------------------------\n    # 2. Tokenize DNA\n    # --------------------------\n    inputs = tokenizer(\n        dna_sequence,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        truncation=True,\n        max_length=tokenizer.model_max_length,\n    )\n\n    input_ids = inputs[\"input_ids\"].to(DEVICE)\n    mask = inputs[\"attention_mask\"].to(DEVICE)\n\n    # --------------------------\n    # 3. Prepare tabular features\n    # --------------------------\n    # If user does not provide scores \u2192 assume dataset mean (0 after normalization)\n    if conservation_scores is None:\n        feats = np.zeros(len(feature_cols), dtype=np.float32)\n    else:\n        feats = np.array(conservation_scores, dtype=np.float32)\n\n    # Inject gnomAD if available\n    for i, c in enumerate(feature_cols):\n        if \"gnomad\" in c.lower():\n            feats[i] = (gnomad_af - feat_means[i]) / feat_stds[i]\n\n    # Normalize all other features\n    feats = (feats - feat_means) / feat_stds\n\n    # Convert safely to tensor\n    features_tensor = torch.from_numpy(feats.astype(np.float32)).unsqueeze(0).to(DEVICE)\n\n    # --------------------------\n    # 4. Run model\n    # --------------------------\n    model.eval()\n    with torch.no_grad():\n        out = model(\n            input_ids=input_ids,\n            attention_mask=mask,\n            features=features_tensor,\n        )\n        prob = torch.softmax(out.logits, dim=1)[0, 1].item()\n\n    # --------------------------\n    # 5. Format clinical output\n    # --------------------------\n    if prob >= 0.9:\n        risk = \"\ud83d\udd34 HIGHLY PATHOGENIC\"\n    elif prob >= 0.7:\n        risk = \"\ud83d\udfe0 LIKELY PATHOGENIC\"\n    elif prob >= 0.3:\n        risk = \"\ud83d\udfe1 UNCERTAIN\"\n    else:\n        risk = \"\ud83d\udfe2 BENIGN\"\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"\ud83e\uddec PATHOPRETER \u2014 VARIANT REPORT\")\n    print(\"=\"*50)\n    print(f\"Sequence (snippet): {dna_sequence[:12]}...{dna_sequence[-12:]}\")\n    print(f\"gnomAD AF:          {gnomad_af:.3e}\")\n    print(f\"Prediction:        {risk}\")\n    print(f\"Pathogenic prob:   {prob:.4f}\")\n    print(\"=\"*50 + \"\\n\")\n\n    return prob\n\n\n# ============================\n# \ud83e\uddea Example tests\n# ============================\n\n# Fake benign DNA (nonsense)\nseq_benign = \"A\" * 80\npredict_single_variant(seq_benign, gnomad_af=0.45)\n\n# Realistic coding DNA-like context (toy example)\nseq_realistic = \"TTGCTGACCTGCTGCTGAGGCTGACTTCAGGTGCTGATGGCTGAGGCTGACT\"\npredict_single_variant(seq_realistic, gnomad_af=0.00001)\n\n",
    "outputs": [
      {
        "type": "stream",
        "text": "\n==================================================\n\ud83e\uddec PATHOPRETER \u2014 VARIANT REPORT\n==================================================\nSequence (snippet): AAAAAAAAAAAA...AAAAAAAAAAAA\ngnomAD AF:          4.500e-01\nPrediction:        \ud83d\udfe2 BENIGN\nPathogenic prob:   0.0000\n==================================================\n\n\n==================================================\n\ud83e\uddec PATHOPRETER \u2014 VARIANT REPORT\n==================================================\nSequence (snippet): TTGCTGACCTGC...CTGAGGCTGACT\ngnomAD AF:          1.000e-05\nPrediction:        \ud83d\udfe2 BENIGN\nPathogenic prob:   0.0018\n==================================================\n\n"
      },
      {
        "type": "result",
        "data": {
          "text/plain": "0.0018386653391644359"
        }
      }
    ]
  },
  {
    "type": "code",
    "source": "# ==========================================\n# \ud83d\udd75\ufe0f\u200d\u2642\ufe0f CELL 8: THE AUTOPSY (Error Analysis) \u2014 FIXED\n# ==========================================\nimport pandas as pd\n\n# 1. Reload Data & Predictions\ndf = pd.read_parquet(\"test_enriched_SEQUENCES.parquet\").reset_index(drop=True)\ndf[\"PathoPreter_Prob\"] = probs  # From previous prediction cell\ndf[\"y_true\"] = y_test           # From previous prediction cell\n\n# 2. Smart Column Detection\n# If 'gene' is missing, try to find a similar column or extract from variant_id\navailable_cols = df.columns.tolist()\ngene_col = \"gene\"\n\nif \"gene\" not in available_cols:\n    # Try common alternatives\n    for alt in [\"gene_symbol\", \"symbol\", \"gene_name\", \"genename\"]:\n        if alt in available_cols:\n            gene_col = alt\n            break\n    else:\n        # Fallback: Extract gene from variant_id if it looks like \"NM_...(GENE):...\"\n        print(\"\u26a0\ufe0f 'gene' column missing. Extracting from variant_id...\")\n        df[\"gene\"] = df[\"variant_id\"].apply(lambda x: x.split(\"(\")[1].split(\")\")[0] if \"(\" in x and \")\" in x else \"Unknown\")\n        gene_col = \"gene\"\n\n# 3. Define Columns to Display (Only use what actually exists)\ncols_to_show = [gene_col, \"variant_id\", \"PathoPreter_Prob\"]\nif \"gnomad_af\" in available_cols:\n    cols_to_show.append(\"gnomad_af\")\n\n# 4. Identify Failures\n# False Negatives: Actual=1, Model said 0 (Prob < 0.2 for \"Confident Miss\")\nfn_mask = (df[\"y_true\"] == 1) & (df[\"PathoPreter_Prob\"] < 0.2)\n\n# False Positives: Actual=0, Model said 1 (Prob > 0.8 for \"Confident Alarm\")\nfp_mask = (df[\"y_true\"] == 0) & (df[\"PathoPreter_Prob\"] > 0.8)\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"From 14000 balanced (1:1 path:benign) test samples\")\nprint(\"=\"*80)\n# 5. Display WORST False Negatives (Dangerous Misses)\nprint(\"\\n\" + \"=\"*80)\nprint(f\"\ud83d\udc80 WORST FALSE NEGATIVES (Dangerous Misses): {fn_mask.sum()} cases\")\nprint(\"The patient is SICK, but model said SAFE with high confidence.\")\nprint(\"=\"*80)\n\nif fn_mask.sum() > 0:\n    # Sort by lowest probability (most confident wrong answer)\n    worst_fn = df[fn_mask].sort_values(\"PathoPreter_Prob\").head(5)\n    print(worst_fn[cols_to_show].to_string(index=False))\nelse:\n    print(\"\u2705 Amazing! No confident False Negatives found.\")\n\n# 6. Display WORST False Positives (False Alarms)\nprint(\"\\n\" + \"=\"*80)\nprint(f\"\ud83d\udea8 WORST FALSE POSITIVES (False Alarms): {fp_mask.sum()} cases\")\nprint(\"The patient is HEALTHY, but model said SICK with high confidence.\")\nprint(\"=\"*80)\n\nif fp_mask.sum() > 0:\n    # Sort by highest probability (most confident wrong answer)\n    worst_fp = df[fp_mask].sort_values(\"PathoPreter_Prob\", ascending=False).head(5)\n    print(worst_fp[cols_to_show].to_string(index=False))\nelse:\n    print(\"\u2705 Amazing! No confident False Positives found.\")\n\nprint(\"\\n\ud83d\udca1 INSIGHT:\")\nprint(\"- If FN are mostly in 'Low-Freq' range, it's likely Recessive Carrier issues.\")\nprint(\"- If FP have high gnomAD, check if they are risk factors rather than direct causes.\")",
    "outputs": [
      {
        "type": "stream",
        "text": "\u26a0\ufe0f 'gene' column missing. Extracting from variant_id...\n\n================================================================================\nFrom 14000 balanced (1:1 path:benign) test samples\n================================================================================\n\n================================================================================\n\ud83d\udc80 WORST FALSE NEGATIVES (Dangerous Misses): 260 cases\nThe patient is SICK, but model said SAFE with high confidence.\n================================================================================\n   gene      variant_id  PathoPreter_Prob  gnomad_af\nUnknown  19_3532086_C_A          0.001082        0.0\nUnknown 6_116120454_A_T          0.001389        0.0\nUnknown  2_47800542_G_T          0.001410        0.0\nUnknown  19_3532086_C_G          0.001598        0.0\nUnknown 17_80110949_C_G          0.002051        0.0\n\n================================================================================\n\ud83d\udea8 WORST FALSE POSITIVES (False Alarms): 769 cases\nThe patient is HEALTHY, but model said SICK with high confidence.\n================================================================================\n   gene       variant_id  PathoPreter_Prob  gnomad_af\nUnknown 10_110780974_A_T          0.999030   0.000000\nUnknown  18_45038505_A_G          0.998675   0.000010\nUnknown   18_2923825_G_A          0.997155   0.000003\nUnknown 10_106706548_G_A          0.997110   0.000205\nUnknown   5_88728563_G_A          0.996972   0.000000\n\n\ud83d\udca1 INSIGHT:\n- If FN are mostly in 'Low-Freq' range, it's likely Recessive Carrier issues.\n- If FP have high gnomAD, check if they are risk factors rather than direct causes.\n"
      }
    ]
  },
  {
    "type": "code",
    "source": "",
    "outputs": []
  },
  {
    "type": "code",
    "source": "",
    "outputs": []
  },
  {
    "type": "code",
    "source": "",
    "outputs": []
  },
  {
    "type": "code",
    "source": "",
    "outputs": []
  },
  {
    "type": "code",
    "source": "",
    "outputs": []
  },
  {
    "type": "code",
    "source": "",
    "outputs": []
  }
]